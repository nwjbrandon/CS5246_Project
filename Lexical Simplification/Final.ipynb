{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kennethshyle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kennethshyle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kennethshyle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word complexity function loaded: <function word_complexity_score at 0x164319580>\n",
      "Sentence complexity function loaded: <function sentence_complexity_score at 0x17e781440>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Import our complexity predictors using importlib for reliability\n",
    "import importlib.util\n",
    "\n",
    "# Import word_complexity module\n",
    "spec = importlib.util.spec_from_file_location(\"word_complexity_mod\", \"word_complexity.py\")\n",
    "word_complexity_mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(word_complexity_mod)\n",
    "WordComplexityPredictor = word_complexity_mod.WordComplexityPredictor\n",
    "word_complexity_score = word_complexity_mod.word_complexity_score\n",
    "\n",
    "# Similarly for sentence_complexity\n",
    "spec = importlib.util.spec_from_file_location(\"sentence_complexity_mod\", \"sentence_complexity.py\")\n",
    "sentence_complexity_mod = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(sentence_complexity_mod)\n",
    "SentenceComplexityPredictor = sentence_complexity_mod.SentenceComplexityPredictor\n",
    "sentence_complexity_score = sentence_complexity_mod.sentence_complexity_score\n",
    "\n",
    "# Print function details to verify they are loaded correctly\n",
    "print(f\"Word complexity function loaded: {word_complexity_score}\")\n",
    "print(f\"Sentence complexity function loaded: {sentence_complexity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ASSET dataset from Hugging Face...\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['original', 'simplifications'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['original', 'simplifications'],\n",
      "        num_rows: 359\n",
      "    })\n",
      "})\n",
      "\n",
      "validation set size: 2000\n",
      "\n",
      "test set size: 359\n",
      "\n",
      "Columns in the dataset:\n",
      "['original', 'simplifications']\n",
      "\n",
      "Sample examples from the validation set:\n",
      "\n",
      "Example 1:\n",
      "Original: Adjacent counties are Marin (to the south), Mendocino (to the north), Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).\n",
      "Simplifications:\n",
      "  1: countries next to it are Marin, Mendocino, Lake, Napa, Solano, and Contra Costa.\n",
      "  2: Nearby counties are Marin, Mendocino, Lake, Napa, and Solano and Contra Costa.\n",
      "\n",
      "Example 2:\n",
      "Original: A Georgian inscription around the drum attests his name.\n",
      "Simplifications:\n",
      "  1: A writing around the drum confirms his name.\n",
      "  2: A Georgian writing on the drum is his name.\n",
      "\n",
      "Example 3:\n",
      "Original: They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", introducing two new variants of the race; the Cyber-Shades and the Cyber-King.\n",
      "Simplifications:\n",
      "  1: They would return to the new series in the 2008 Special \"The Next Doctor\". There were two new variants; Cyber-Shades and the Cyber-King.\n",
      "  2: They returned to the series in the 2008 Christmas Special \"The Next Doctor\". And started two new races; the Cyber-Shades and the Cyber-King.\n"
     ]
    }
   ],
   "source": [
    "# 2. Load and Explore the ASSET Dataset\n",
    "print(\"Loading ASSET dataset from Hugging Face...\")\n",
    "asset = load_dataset(\"facebook/asset\")\n",
    "# Print basic information about the dataset\n",
    "print(\"\\nDataset structure:\")\n",
    "print(asset)\n",
    "# Inspect the available splits\n",
    "for split in asset:\n",
    "    print(f\"\\n{split} set size:\", len(asset[split]))\n",
    "    \n",
    "# Get column information\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(asset[\"validation\"].column_names)\n",
    "\n",
    "# Display a few examples - PROPERLY ACCESSING THE DATASET\n",
    "print(\"\\nSample examples from the validation set:\")\n",
    "validation_data = asset[\"validation\"]\n",
    "for i in range(3):  # Get the first 3 examples\n",
    "    example = validation_data[i]  # Get by index\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {example['original']}\")\n",
    "    print(f\"Simplifications:\")\n",
    "    # Get the first 2 simplifications for each example\n",
    "    for j, simplification in enumerate(example['simplifications'][:2]):\n",
    "        print(f\"  {j+1}: {simplification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   example_id   text_type  \\\n",
      "0           0    Original   \n",
      "1           0  Simplified   \n",
      "2           1    Original   \n",
      "3           1  Simplified   \n",
      "4           2    Original   \n",
      "5           2  Simplified   \n",
      "6           3    Original   \n",
      "7           3  Simplified   \n",
      "8           4    Original   \n",
      "9           4  Simplified   \n",
      "\n",
      "                                                                                                  text  \\\n",
      "0  Adjacent counties are Marin (to the south), Mendocino (to the north), Lake (northeast), Napa (to...   \n",
      "1                     countries next to it are Marin, Mendocino, Lake, Napa, Solano, and Contra Costa.   \n",
      "2                                             A Georgian inscription around the drum attests his name.   \n",
      "3                                                         A writing around the drum confirms his name.   \n",
      "4  They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", i...   \n",
      "5  They would return to the new series in the 2008 Special \"The Next Doctor\". There were two new va...   \n",
      "6  Jameson's autobiography, How to Make Love Like a Porn Star: A Cautionary Tale was published Augu...   \n",
      "7                                             Jameson's autobiography was published on August 17, 2004   \n",
      "8                                          It is particularly famous for the cultivation of kiwifruit.   \n",
      "9                                                      It is famous for the cultivation of kiwi fruit.   \n",
      "\n",
      "   sentence_complexity  avg_word_complexity  \n",
      "0            26.050000            33.447983  \n",
      "1            27.142308            39.080866  \n",
      "2            24.438889            36.635828  \n",
      "3            20.262500            33.315034  \n",
      "4            24.183333            31.146884  \n",
      "5            20.410000            29.652632  \n",
      "6            23.813158            40.812032  \n",
      "7            25.605556            49.038901  \n",
      "8            32.716667            44.988297  \n",
      "9            22.938889            35.090281  \n",
      "\n",
      "Average sentence complexity reduction: 2.97 points (11.31%)\n",
      "\n",
      "Word-by-word complexity breakdown for first example:\n",
      "Original text:\n",
      "Adjacent counties are Marin (to the south), Mendocino (to the north), Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).\n",
      "\n",
      "Complex words (complexity > 50):\n",
      "  - 'Mendocino': 63.0\n",
      "  - 'Adjacent': 56.8\n",
      "  - 'Solano': 51.2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAIhCAYAAABwnkrAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZA0lEQVR4nO3deVRV5f7H8c8RGWVQMAYVgUoNRZwzNBM0x7QcSptU0sxKK6dMMxUtxTJNG7SbXafKqZt6y8p5Tu2q5ZCapWlaQs5iDpDy/P5ocX6eDRpHgYP4fq111mI/+9l7f9lnw+LDs/dzbMYYIwAAAACAXTFXFwAAAAAAhQ1BCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJwBV9++23atu2rcqXLy9PT0+FhIQoLi5O/fr1y9fjnjt3TklJSVq1alW+HqegpaWlaeTIkapdu7b8/f3l6empyMhIde3aVd99952ry7uqadOmyWaz6cCBA/l2jPj4eMXHx9uXC/o6+OuvvzRp0iTFxcUpICBA3t7eio6O1sCBA3X8+HGn9hUZGanExMRrqiMxMVGRkZHXtG1uHThwQDabTdOmTcvX4+TG7t271alTJ916663y8vJS6dKlVbNmTfXq1UtpaWn2fgVxXq7GZrMpKSnJvrxq1SrZbLZs1+c777yj22+/XR4eHrLZbDp16lS+1G79eQGQ94q7ugAAhdOXX36p+++/X/Hx8XrjjTcUFhamlJQUbd68WbNnz9bYsWPz7djnzp3T8OHDJanI/CGwb98+NW3aVEeOHNHTTz+t4cOHy9fXVwcOHNDcuXNVq1YtnTp1SgEBAa4u1WUmTpzosFyQ18G5c+fUsmVLrVu3Tk899ZSGDBkib29vbdiwQW+++aZmzpyppUuXqlKlSrna3/z58+Xv739NtQwZMkQvvPDCNW17o/n+++9Vv359RUdHa+jQoYqMjNSxY8e0bds2zZ49W/3797efx8J2XmrWrKkNGzaocuXK9ratW7fq+eef15NPPqkuXbqoePHi8vPzK3S1A8gdghKAHL3xxhuKiorS4sWLVbz4//+qePjhh/XGG2+4sLIbz6VLl9S2bVsdO3ZMGzZsUExMjH1dw4YN1aVLF3399ddyd3d3YZWud/kfnAWtT58+Wr16tWbPnq2OHTva2xMSEvTggw/qzjvvVPv27bVt2za5ubldcT/nz5+Xt7e3atSocc213Hbbbde87Y1m/PjxKlasmFatWiU/Pz97+4MPPqhXX31Vxhh7W2E7L/7+/rrrrrsc2nbu3ClJ6t69u+688057e2GrHUDucOsdgBwdP35cpUuXdghJWYoVy/6rY86cOYqLi1OJEiXk6+urZs2a6fvvv3fok5iYKF9fX+3du1ctW7aUr6+vwsPD1a9fP6Wnp0v6+5agW265RZI0fPhw2Ww22Ww2h9uYfv75Zz366KMKDg6Wp6enoqOj9d577zkcK+u2mFmzZmnw4MEqU6aM/P39de+992rPnj3Z6l+0aJEaN26sgIAA+fj4KDo6WsnJyQ59Nm/erPvvv1+BgYHy8vJSjRo1NHfu3H88lwsWLNCOHTs0aNAgh5B0uRYtWsjHx8e+vG7dOjVu3Fh+fn7y8fFRvXr19OWXXzpsk3U73IoVK9S9e3cFBQXJ399fnTt31tmzZ5WamqoOHTqoZMmSCgsLU//+/fXXX3/Zt8+6/eqNN97QyJEjVb58eXl5eal27dpavnz5P35fkrRs2TI1btxY/v7+8vHxUf369R22/fnnn+Xv76+HHnrIYbsVK1bIzc1NQ4YMsbddfivR1a6DtWvX2t9bqxkzZshms2nTpk25ql+SUlNTNWXKFDVr1swhJGWpWLGiXnrpJe3cuVMLFiywt0dGRqpVq1aaN2+eatSoIS8vL/sIWE633u3cuVNNmzaVj4+PbrnlFvXs2VNffvllttu3crpNy2azqVevXvroo48UHR0tHx8fVatWTQsXLnTot3fvXj3xxBOqUKGCfHx8VLZsWbVu3Vo7duzI9fnIcvToUXl4eDi8R1l+/PFH2Ww2vf3225L+HpHr37+/oqKi5OXlpcDAQNWuXTvH9+hyx48fl7+/v3x9fXNcb7PZ7F9f7bxMnTpVlSpVkre3t2rXrq2NGzfKGKMxY8YoKipKvr6+atSokfbu3euwfXx8vGJiYrR27Vrddddd8vb2VtmyZTVkyBBdunTpqrVbb72Lj4/X448/LkmqW7euw++tnGo3xmjixImqXr26vL29VapUKT344IP65ZdfsvV74403FBERIS8vL9WsWVNff/31VWsDkEcMAOTgySefNJLMc889ZzZu3GgyMjKu2HfkyJHGZrOZrl27moULF5p58+aZuLg4U6JECbNz5057vy5duhgPDw8THR1t3nzzTbNs2TIzdOhQY7PZzPDhw40xxly4cMEsWrTISDLdunUzGzZsMBs2bDB79+41xhizc+dOExAQYKpWrWpmzJhhlixZYvr162eKFStmkpKS7MdauXKlkWQiIyPNY489Zr788ksza9YsU758eVOhQgVz8eJFe98PP/zQ2Gw2Ex8fb2bOnGmWLVtmJk6caJ599ll7nxUrVhgPDw/ToEEDM2fOHLNo0SKTmJhoJJmpU6de9Vw+9dRTRpLZvXt3rs79qlWrjLu7u6lVq5aZM2eOWbBggWnatKmx2Wxm9uzZ9n5Tp041kkxUVJTp16+fWbJkiXn99deNm5ubeeSRR0zNmjXNa6+9ZpYuXWpeeuklI8mMHTvWvv3+/fuNJBMeHm7uvvtu89lnn5lPP/3U1KlTx7i7u5v169dnO9b+/fvtbR999JGx2WymTZs2Zt68eeaLL74wrVq1Mm5ubmbZsmX2frNnzzaSzIQJE4wxxqSkpJiQkBDTsGFDh/ehYcOGpmHDhsaYf74OatSoYerXr5/t3NWpU8fUqVPHoeZ/en9mzpxpJJlJkyZdsc+uXbuMJNOjRw97W0REhAkLCzO33nqrmTJlilm5cqX53//+Z1/XpUsXe9/Dhw+boKAgU758eTNt2jTz1VdfmU6dOpnIyEgjyaxcudLet0uXLiYiIsLh+FnX8p133mnmzp1rvvrqKxMfH2+KFy9u9u3bZ++3evVq069fP/Of//zHrF692syfP9+0adPGeHt7mx9//NHeL+u9/6dz07ZtWxMeHm4uXbrk0D5gwADj4eFhjh07ZowxpkePHsbHx8eMGzfOrFy50ixcuNCMHj3avPPOO1fd/2uvvWYkmUceecSsWrXKnDt37op9r3ReIiIiTL169cy8efPM/PnzTcWKFU1gYKDp06ePeeCBB8zChQvNJ598YkJCQkxsbKzJzMy0b9+wYUMTFBRkypQpY95++22zePFi8/zzzxtJpmfPntmONWzYMPty1u+YrPdu586d5pVXXrGf18uv15xq7969u3F3dzf9+vUzixYtMjNnzjR33HGHCQkJMampqfZ+w4YNs/8cfP311+aDDz4wZcuWNaGhofafFwD5g6AEIEfHjh0zd999t5FkJBl3d3dTr149k5ycbM6cOWPvd/DgQVO8eHHz3HPPOWx/5swZExoaajp06GBv69Kli5Fk5s6d69C3ZcuWplKlSvblo0ePZvujJEuzZs1MuXLlzOnTpx3ae/XqZby8vMyJEyeMMf//R0zLli0d+s2dO9dIMhs2bLDX6e/vb+6++26HP6Cs7rjjDlOjRg3z119/ObS3atXKhIWFZftD8nLNmzc3ksyFCxeu2Odyd911lwkODnY4zxcvXjQxMTGmXLly9jqzgoD13Ldp08ZIMuPGjXNor169uqlZs6Z9OeuP5TJlypjz58/b29PS0kxgYKC599577W3WoHT27FkTGBhoWrdu7XCMS5cumWrVqpk777zTof2ZZ54xHh4eZsOGDaZRo0YmODjYHD582KHP5UHJmKtfB1n1fP/99/a2//3vf0aSmT59ujHGmOnTpxs3Nzf78pWMHj3aSDKLFi26Yp/z588bSaZFixb2toiICOPm5mb27NmTrb81KL344ovGZrM5/OPAmL+v59wGpZCQEJOWlmZvS01NNcWKFTPJyclXrPvixYsmIyPDVKhQwfTp08fentug9PnnnxtJZsmSJQ77LFOmjGnfvr29LSYmxrRp0+aq+8rJhQsX7NerJOPm5mZq1KhhBg8ebI4cOeLQ90rnJTQ01Pz555/2tgULFhhJpnr16g4/0+PHjzeSzPbt2+1tDRs2NJLMf//7X4f9du/e3RQrVsz8+uuvDse6WlAy5v+vy02bNl219g0bNmT7x4Uxxhw6dMh4e3ubAQMGGGOMOXnypPHy8jJt27Z16PfNN98YSQQlIJ9x6x2AHAUFBWnt2rXatGmTRo8erQceeEA//fSTBg0apKpVq+rYsWOSpMWLF+vixYvq3LmzLl68aH95eXmpYcOG2WaEstlsat26tUNbbGysfv3113+s6cKFC1q+fLnatm0rHx8fh+O1bNlSFy5c0MaNGx22uf/++7MdS5L9eOvXr1daWpqeffZZh9t8Lrd37179+OOPeuyxxyQp23FTUlJyvJ3vWpw9e1bffvutHnzwQYfbkdzc3NSpUyf99ttv2Y7VqlUrh+Xo6GhJ0n333ZetPafz3K5dO3l5edmX/fz81Lp1a61Zs+aKtx+tX79eJ06cUJcuXRzOR2Zmppo3b65Nmzbp7Nmz9v5vvfWWqlSpooSEBK1atUoff/yxwsLCcnlWsnvkkUcUHBzscMvlO++8o1tuucV++1zWNdm5c+drPo6V9RqJjY1VxYoV/3G71atXKyYmJttzWI888kiuj52QkODwHE9ISIiCg4Md3tOLFy9q1KhRqly5sjw8PFS8eHF5eHjo559/1u7du3N9rCwtWrRQaGiopk6dam9bvHixDh8+rK5du9rb7rzzTn399dcaOHCgVq1apfPnz+dq/56enpo/f7527dqlt956Sw8//LCOHj2qkSNHKjo6Olc/VwkJCSpRooR9Oev6b9GihcP7ldVu/Rnw8/PL9nvi0UcfVWZmptasWZOr78NZCxculM1m0+OPP+7w8xMaGqpq1arZf29u2LBBFy5csP/uyVKvXj1FRETkS20A/h9BCcBV1a5dWy+99JI+/fRTHT58WH369NGBAwfsEzr88ccfkqQ6derI3d3d4TVnzhx7oMri4+Pj8Ee59PcfSxcuXPjHWo4fP66LFy/qnXfeyXasli1bSlK24wUFBWU7liT7H3JHjx6VJJUrV+6Kx836Hvv375/tuM8++2yOx71c+fLlJUn79+//x+/x5MmTMsbkGCLKlCkjSdmmqg4MDHRY9vDwuGJ7Tuc5NDQ0x7aMjAz9+eefOdaZdU4efPDBbOfk9ddflzFGJ06csPf39PTUo48+qgsXLqh69epq0qRJjvvNLU9PT/Xo0UMzZ87UqVOndPToUc2dO1dPPvmk/T3Ordy8P1nrwsPDHdpzG/aOHz+ukJCQbO05tV2J9VqW/j4Pl4eSvn37asiQIWrTpo2++OILffvtt9q0aZOqVauW6/ByueLFi6tTp06aP3++Tp06JenvZ+PCwsLUrFkze7+3335bL730khYsWKCEhAQFBgaqTZs2+vnnn3N1nOjoaPXu3Vsff/yxDh48qHHjxun48eM5Ph9l5cz1Lynbz0BO70HWz4Sz08Ln1h9//CFjjEJCQrL9/GzcuNH++yTr+Ff6GQWQv5j1DkCuubu7a9iwYXrrrbf0ww8/SJJKly4tSfrPf/6T7//hLFWqlH1kpWfPnjn2iYqKcmqfWRMG/Pbbb1fsk/U9Dho0SO3atcuxz9WmjW7WrJk++OADLViwQAMHDrxqPaVKlVKxYsWUkpKSbd3hw4cd6skrqampObZ5eHhc8SH7rBreeeedbDN/Zbn8D9AffvhBQ4cOVZ06dbRp0yaNGzdOffv2va66n3nmGY0ePVpTpkzRhQsXdPHiRT399NNO7ychIUHFixfXggULrrh91iQO1oB3pVFIq6CgIHu4vFxO5/56fPzxx+rcubNGjRrl0H7s2DGVLFnymvb5xBNPaMyYMfYZAT///HP17t3bYfa/EiVKaPjw4Ro+fLj++OMP++hS69at9eOPPzp1PJvNpj59+mjEiBH23zP56WrvS07hNC+ULl1aNptNa9euzTHYZ7VlHf9KP6Ou/Fwp4GZAUAKQo5SUlBz/W551+07W6EazZs1UvHhx7du3T+3bt8+TY1tHfbL4+PgoISFB33//vWJjY+3/Ib4e9erVU0BAgN5//309/PDDOf7hW6lSJVWoUEHbtm3L9gdobjzwwAOqWrWqkpOT1apVqxxnvlu8eLEaNGigEiVKqG7dupo3b57efPNNeXt7S5IyMzP18ccfq1y5crm61csZ8+bN05gxY+wjfWfOnNEXX3yhBg0aXHEq7Pr166tkyZLatWuXevXqddX9nz17Vg899JAiIyO1cuVKDRw4UAMHDlT9+vVVt27dK253pesgS1hYmB566CFNnDhRGRkZat26tX10yBmhoaHq2rWrPvjgA82ZMyfbzHc//fSTXn/9dVWpUkVt2rRxev/S39PAv/nmm9q1a5fD7XezZ8++pv1dic1my/aH95dffqnff/9dt99++zXtMzo6WnXr1tXUqVN16dIlpaen64knnrhi/5CQECUmJmrbtm0aP368zp075zCj4+Wu9Hvm8OHDSktLU61ata6pZmecOXNGn3/+ucPtdzNnzlSxYsV0zz335MsxW7VqpdGjR+v3339Xhw4drtjvrrvukpeXlz755BOH36/r16/Xr7/+SlAC8hlBCUCOmjVrpnLlyql169a64447lJmZqa1bt2rs2LHy9fW1f3hiZGSkRowYocGDB+uXX35R8+bNVapUKf3xxx/63//+Z/9PszP8/PwUERGh//73v2rcuLECAwNVunRpRUZGasKECbr77rvVoEEDPfPMM4qMjNSZM2e0d+9effHFF1qxYoVTx/L19dXYsWP15JNP6t5771X37t0VEhKivXv3atu2bXr33XclSf/617/UokULNWvWTImJiSpbtqxOnDih3bt367vvvtOnn356xWO4ublp/vz5atq0qeLi4vTMM8/Yn6v49ddf9Z///EdffPGFTp48KUlKTk5WkyZNlJCQoP79+8vDw0MTJ07UDz/8oFmzZuV6FCO33Nzc1KRJE/Xt21eZmZl6/fXXlZaWdtX3zdfXV++88466dOmiEydO6MEHH1RwcLCOHj2qbdu26ejRo5o0aZIk6emnn9bBgwft18PYsWO1YcMGPfzww/r++++vONJxtesgywsvvGAPW5c/RyP9PVV4165dNWXKlH98TmncuHHas2ePHn/8ca1Zs0atW7eWp6enNm7cqDfffFN+fn767LPPrvoZSlfTu3dvTZkyRS1atNCIESMUEhKimTNn2kdbcppy/1q0atVK06ZN0x133KHY2Fht2bJFY8aMueqtpbnRtWtX9ejRQ4cPH1a9evWyjaDWrVtXrVq1UmxsrEqVKqXdu3fro48+Ulxc3BVDkiQ99dRTOnXqlNq3b6+YmBi5ubnpxx9/1FtvvaVixYrppZdeuq66cyMoKEjPPPOMDh48qIoVK+qrr77S5MmT9cwzz1xT8M6N+vXr66mnntITTzyhzZs365577lGJEiWUkpKidevWqWrVqnrmmWdUqlQp9e/fX6+99pqefPJJPfTQQzp06JCSkpK49Q4oCC6eTAJAITVnzhzz6KOPmgoVKhhfX1/j7u5uypcvbzp16mR27dqVrf+CBQtMQkKC8ff3N56eniYiIsI8+OCDDtNEd+nSxZQoUSLbtlnT315u2bJlpkaNGsbT09NIcphBbP/+/aZr166mbNmyxt3d3dxyyy2mXr165rXXXrP3yZqR6tNPP3XY75Vm+/rqq69Mw4YNTYkSJYyPj4+pXLmyef311x36bNu2zXTo0MEEBwcbd3d3Exoaaho1amTef//9fzyfxhhz6tQp8+qrr5qaNWs6nNPHH3/cfPPNNw59165daxo1amRKlChhvL29zV133WW++OILhz5XmmEr63wePXrUod16/rPOxeuvv26GDx9uypUrZzw8PEyNGjXM4sWLczzW5dODG/P3dNT33XefCQwMNO7u7qZs2bLmvvvus5/3yZMn53i+9+7da/z9/R1mSrPOemfM1a+DLJGRkSY6Ojpbe26nB8+SkZFh3nvvPVO3bl3j6+trPD09TaVKlcyAAQPs02BfLiIiwtx333057ss6650xxvzwww/m3nvvNV5eXiYwMNB069bNTJ8+3Ugy27Zts/e70uxu1umqczrOyZMnTbdu3UxwcLDx8fExd999t1m7dm22c5vbWe+ynD592nh7extJZvLkydnWDxw40NSuXduUKlXKeHp6mltvvdX06dMnx/N2ucWLF5uuXbuaypUrm4CAAFO8eHETFhZm2rVrZ5+ZMktuz0vW9zZmzBiH9px+JzRs2NBUqVLFrFq1ytSuXdt4enqasLAw8/LLL2eb4VJ5OOtdlilTppi6devaf85vu+0207lzZ7N582Z7n8zMTJOcnGzCw8ONh4eHiY2NNV988UWOPy8A8pbNmMs+9hoAcNM4cOCAoqKiNGbMGPXv39/V5VyT7du3q1q1anrvvffsE2vcSJ566inNmjVLx48fz5NbSeGc+Ph4HTt2rECehQJw4+HWOwDADWffvn369ddf9fLLLyssLEyJiYmuLukfjRgxQmXKlNGtt96qP//8UwsXLtSHH36oV155hZAEAIUQQQkAcMN59dVX9dFHHyk6OlqffvrpVZ+DKSzc3d01ZswY/fbbb7p48aIqVKigcePG2Z/3AwAULtx6BwAAAAAWfOAsAAAAAFgQlAAAAADAgqAEAAAAABZFfjKHzMxMHT58WH5+fnn+IY0AAAAAbhzGGJ05c0ZlypT5xw/7LvJB6fDhwwoPD3d1GQAAAAAKiUOHDqlcuXJX7VPkg5Kfn5+kv0+Gv7+/i6sBAAAA4CppaWkKDw+3Z4SrKfJBKet2O39/f4ISAAAAgFw9ksNkDgAAAABgQVACAAAAAAuCEgAAAABYFPlnlHLDGKOLFy/q0qVLri4FecDNzU3FixdnOngAAABcs5s+KGVkZCglJUXnzp1zdSnIQz4+PgoLC5OHh4erSwEAAMAN6KYOSpmZmdq/f7/c3NxUpkwZeXh4MApxgzPGKCMjQ0ePHtX+/ftVoUKFf/wwMQAAAMDqpg5KGRkZyszMVHh4uHx8fFxdDvKIt7e33N3d9euvvyojI0NeXl6uLgkAAAA3GP7VLjHiUATxngIAAOB68NckAAAAAFgQlAAAAADAgqAEAAAAABYEpQIWHx+v3r17u3Sfq1atks1m06lTp/K0DgAAAKCoICgBAAAAgAVBqQAlJiZq9erVmjBhgmw2m2w2mw4cOKBdu3apZcuW8vX1VUhIiDp16qRjx45J+nv0x8PDQ2vXrrXvZ+zYsSpdurRSUlKuuM8rOXDggBISEiRJpUqVks1mU2JiombMmKGgoCClp6c79G/fvr06d+4sSUpKSlL16tX1r3/9yz6l+kMPPZRtZGrq1KmKjo6Wl5eX7rjjDk2cODEPzh4AAABQcAhKBWjChAmKi4tT9+7dlZKSopSUFLm7u6thw4aqXr26Nm/erEWLFumPP/5Qhw4dJP3/bXWdOnXS6dOntW3bNg0ePFiTJ09WWFhYjvsMDw+/Yg3h4eH67LPPJEl79uxRSkqKJkyYoIceekiXLl3S559/bu977NgxLVy4UE888YS9be/evZo7d66++OILLVq0SFu3blXPnj3t6ydPnqzBgwdr5MiR2r17t0aNGqUhQ4Zo+vTpeX06AQAAgHxzU3/gbEELCAiQh4eHfHx8FBoaKkkaOnSoatasqVGjRtn7TZkyReHh4frpp59UsWJFvfbaa1q2bJmeeuop7dy5U506dVLbtm2vuM+rcXNzU2BgoCQpODhYJUuWtK979NFHNXXqVD300EOSpE8++UTlypVTfHy8vc+FCxc0ffp0lStXTpL0zjvv6L777tPYsWMVGhqqV199VWPHjlW7du0kSVFRUdq1a5f+9a9/qUuXLtd+8gAAAIACRFBysS1btmjlypXy9fXNtm7fvn2qWLGiPDw89PHHHys2NlYREREaP358vtTSvXt31alTR7///rvKli2rqVOnKjExUTabzd6nfPny9pAkSXFxccrMzNSePXvk5uamQ4cOqVu3burevbu9z8WLFxUQEJAvNQMAAAD5gaDkYpmZmWrdurVef/31bOvCwsLsX69fv16SdOLECZ04cUIlSpTI81pq1KihatWqacaMGWrWrJl27NihL7744qrbZIUom82mzMxMSX/ffle3bl2Hfm5ubnleLwAAAJBfCEoFzMPDQ5cuXbIv16xZU5999pkiIyNVvHjOb8e+ffvUp08fTZ48WXPnzlXnzp21fPlyFStWLMd95qYGSTlu8+STT+qtt97S77//rnvvvTfb804HDx7U4cOHVaZMGUnShg0bVKxYMVWsWFEhISEqW7asfvnlFz322GO5rgcAAAAobAhKBSwyMlLffvutDhw4IF9fX/Xs2VOTJ0/WI488ohdffFGlS5fW3r17NXv2bE2ePFmS1KlTJzVt2lRPPPGEWrRooapVq2rs2LF68cUXc9xnYGCgPUTlJCIiQjabTQsXLlTLli3l7e1tv/XvscceU//+/TV58mTNmDEj27ZeXl7q0qWL3nzzTaWlpen5559Xhw4d7M9HJSUl6fnnn5e/v79atGih9PR0bd68WSdPnlTfvn3z+nQCAABIkmq9mP3vlpvBljGdXV1CkcWsdwWsf//+cnNzU+XKlXXLLbcoIyND33zzjS5duqRmzZopJiZGL7zwggICAlSsWDGNHDlSBw4c0AcffCBJCg0N1YcffqhXXnlFW7duzXGfBw8evGoNZcuW1fDhwzVw4ECFhISoV69e9nX+/v5q3769fH191aZNm2zb3n777WrXrp1atmyppk2bKiYmxmH67yeffFIffvihpk2bpqpVq6phw4aaNm2aoqKirv/kAQAAAAXEZowxri4iP6WlpSkgIECnT5+Wv7+/w7oLFy5o//79ioqKkpeXl4sqLHyaNGmi6Ohovf322w7tSUlJWrBggT2gFWa8twAA3FwYUUJuXC0bWHHrHexOnDihJUuWaMWKFXr33XddXQ4AAADgMtx6VwQ9/fTT8vX1zfH19NNPX3G7mjVrqkePHnr99ddVqVKlAqwYAAAAKFy49a4I3p515MgRpaWl5bjO399fwcHBBVxRwSuq7y0AAMgZt94hN7j17iYXHBx8U4QhAAAAIL9w6x0AAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUHpJnTgwAHZbDZt3bo119tMmzZNJUuWdHkdAAAAQEFgevArKMi5+K91/vtDhw4pKSlJX3/9tY4dO6awsDC1adNGQ4cOVVBQ0BW3Cw8PV0pKikqXLp3rY3Xs2FEtW7a8pjoBAACAGw0jSjeoX375RbVr19ZPP/2kWbNmae/evXr//fe1fPlyxcXF6cSJEzlul5GRITc3N4WGhqp48dznZG9vbz6bCQAAADcNgtINqmfPnvLw8NCSJUvUsGFDlS9fXi1atNCyZcv0+++/a/DgwZKkyMhIvfbaa0pMTFRAQIC6d++e4y1vn3/+uSpUqCBvb28lJCRo+vTpstlsOnXqlKTst94lJSWpevXq+uijjxQZGamAgAA9/PDDOnPmjL3PokWLdPfdd6tkyZIKCgpSq1attG/fvoI4PQAAAMB1ISjdgE6cOKHFixfr2Weflbe3t8O60NBQPfbYY5ozZ46MMZKkMWPGKCYmRlu2bNGQIUOy7e/AgQN68MEH1aZNG23dulU9evSwB62r2bdvnxYsWKCFCxdq4cKFWr16tUaPHm1ff/bsWfXt21ebNm3S8uXLVaxYMbVt21aZmZnXeQYAAACA/MUzSjegn3/+WcYYRUdH57g+OjpaJ0+e1NGjRyVJjRo1Uv/+/e3rDxw44ND//fffV6VKlTRmzBhJUqVKlfTDDz9o5MiRV60jMzNT06ZNk5+fnySpU6dOWr58uX279u3bO/T/97//reDgYO3atUsxMTG5/4YBAACAAsaIUhGUNZJks9kkSbVr175q/z179qhOnToObXfeeec/HicyMtIekiQpLCxMR44csS/v27dPjz76qG699Vb5+/srKipKknTw4MHcfSMAAACAixCUbkC33367bDabdu3aleP6H3/8UaVKlbLPaleiRImr7s8YYw9Vl7f9E3d3d4dlm83mcFtd69atdfz4cU2ePFnffvutvv32W0l/TygBAAAAFGYEpRtQUFCQmjRpookTJ+r8+fMO61JTU/XJJ5+oY8eO2cLPldxxxx3atGmTQ9vmzZuvq8bjx49r9+7deuWVV9S4cWP77YAAAADAjYCgdIN69913lZ6ermbNmmnNmjU6dOiQFi1apCZNmqhs2bL/+HzR5Xr06KEff/xRL730kn766SfNnTtX06ZNk6Rchy2rUqVKKSgoSB988IH27t2rFStWqG/fvte0LwAAAKCgMZnDFVzrh8AWlAoVKmjz5s1KSkpSx44ddfz4cYWGhqpNmzYaNmyYAgMDc72vqKgo/ec//1G/fv00YcIExcXFafDgwXrmmWfk6el5TfUVK1ZMs2fP1vPPP6+YmBhVqlRJb7/9tuLj469pfwAAAEBBspncPIxyA0tLS1NAQIBOnz4tf39/h3UXLlzQ/v37FRUVJS8vLxdVWDiNHDlS77//vg4dOuTqUq4J7y0AADeXWi/OcHUJLlHY/7lf2FwtG1gxogRJ0sSJE1WnTh0FBQXpm2++0ZgxY9SrVy9XlwUAAAC4BEEJkv7+bKbXXntNJ06cUPny5dWvXz8NGjTI1WUBAAAALkFQgiTprbfe0ltvveXqMgAAAIBCgVnvAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACxcGpQmTZqk2NhY+fv7y9/fX3Fxcfr666/t640xSkpKUpkyZeTt7a34+Hjt3LnThRUDAAAAuBm4NCiVK1dOo0eP1ubNm7V582Y1atRIDzzwgD0MvfHGGxo3bpzeffddbdq0SaGhoWrSpInOnDnjyrJvCDabTQsWLMj348THx6t379725cjISI0fP96+nJqaqiZNmqhEiRIqWbJkntWWmJioNm3aXNc+AAAAgCtx6ecotW7d2mF55MiRmjRpkjZu3KjKlStr/PjxGjx4sNq1aydJmj59ukJCQjRz5kz16NEjX2s7OKJqvu7/cuWH7nB6myNHjmjIkCH6+uuv9ccff6hUqVKqVq2akpKSFBcXp5SUFJUqVSofqr26TZs2qUSJEvblt956SykpKdq6dasCAgIkyWW1AQAAALlVaD5w9tKlS/r000919uxZxcXFaf/+/UpNTVXTpk3tfTw9PdWwYUOtX7/+ikEpPT1d6enp9uW0tLR8r90V2rdvr7/++kvTp0/Xrbfeqj/++EPLly/XiRMnJEmhoaEuqeuWW25xWN63b59q1aqlChUq2NtcVRsAAACQWy6fzGHHjh3y9fWVp6ennn76ac2fP1+VK1dWamqqJCkkJMShf0hIiH1dTpKTkxUQEGB/hYeH52v9rnDq1CmtW7dOr7/+uhISEhQREaE777xTgwYN0n333SfJ8fa2AwcOyGazae7cuWrQoIG8vb1Vp04d/fTTT9q0aZNq164tX19fNW/eXEePHrUfJ+v2tuHDhys4OFj+/v7q0aOHMjIyrljb5bfeRUZG6rPPPtOMGTNks9mUmJiYrTZJ+v3339WxY0eVKlVKQUFBeuCBB3TgwAH7+kuXLqlv374qWbKkgoKCNGDAABlj8uRcAgAAADlxeVCqVKmStm7dqo0bN+qZZ55Rly5dtGvXLvt6m83m0N8Yk63tcoMGDdLp06ftr0OHDuVb7a7i6+srX19fLViwwGH07J8MGzZMr7zyir777jsVL15cjzzyiAYMGKAJEyZo7dq12rdvn4YOHeqwzfLly7V7926tXLlSs2bN0vz58zV8+PBcHW/Tpk1q3ry5OnTooJSUFE2YMCFbn3PnzikhIUG+vr5as2aN1q1bZw9tWYFs7NixmjJliv79739r3bp1OnHihObPn5/r7xsAAABwlsuDkoeHh26//XbVrl1bycnJqlatmiZMmGC/Pcs6enTkyJFso0yX8/T0tM+il/UqaooXL65p06Zp+vTpKlmypOrXr6+XX35Z27dvv+p2/fv3V7NmzRQdHa0XXnhB3333nYYMGaL69eurRo0a6tatm1auXOmwjYeHh6ZMmaIqVarovvvu04gRI/T2228rMzPzH+u85ZZb5OnpKW9vb4WGhtqfUbrc7NmzVaxYMX344YeqWrWqoqOjNXXqVB08eFCrVq2SJI0fP16DBg1S+/btFR0drffffz/HfQEAAAB5xeVBycoYo/T0dEVFRSk0NFRLly61r8vIyNDq1atVr149F1ZYOLRv316HDx/W559/rmbNmmnVqlWqWbOmpk2bdsVtYmNj7V9nhc2qVas6tB05csRhm2rVqsnHx8e+HBcXpz///DPPRuq2bNmivXv3ys/Pzz5SFhgYqAsXLmjfvn06ffq0UlJSFBcXZ9+mePHiql27dp4cHwAAAMiJSydzePnll9WiRQuFh4frzJkzmj17tlatWqVFixbJZrOpd+/eGjVqlCpUqKAKFSpo1KhR8vHx0aOPPurKsgsNLy8vNWnSRE2aNNHQoUP15JNPatiwYfZngazc3d3tX2fdvmhty81I0eXbX6/MzEzVqlVLn3zySbZ11okhAAAAgILi0qD0xx9/qFOnTkpJSVFAQIBiY2O1aNEiNWnSRJI0YMAAnT9/Xs8++6xOnjypunXrasmSJfLz83Nl2YVW5cqV8/yzk7Zt26bz58/L29tbkrRx40b5+vqqXLlyebL/mjVras6cOfbJInISFhamjRs36p577pEkXbx4UVu2bFHNmjXzpAYAAADAyqVB6d///vdV19tsNiUlJSkpKalgCrpBHD9+XA899JC6du2q2NhY+fn5afPmzXrjjTf0wAMP5OmxMjIy1K1bN73yyiv69ddfNWzYMPXq1UvFiuXNXZuPPfaYxowZowceeEAjRoxQuXLldPDgQc2bN08vvviiypUrpxdeeEGjR49WhQoVFB0drXHjxunUqVN5cnwAAAAgJ4Xmc5QKm2v5ENiC4uvrq7p16+qtt97Svn379Ndffyk8PFzdu3fXyy+/nKfHaty4sSpUqKB77rlH6enpevjhh/M0uPr4+GjNmjV66aWX1K5dO505c0Zly5ZV48aN7SNM/fr1U0pKihITE1WsWDF17dpVbdu21enTp/OsDgAAAOByNlPEP5AmLS1NAQEBOn36dLZbuy5cuKD9+/crKipKXl5eLqqw8EpMTNSpU6fy/Ha+gsB7CwDAzaXWizNcXYJLbBnT2dUl3FCulg2sCt2sdwAAAADgagQlAAAAALDgGSVc0dU+kwkAAAAoyhhRAgAAAAALgpKkIj6fxU2J9xQAAADX46YOSu7u7pKkc+fOubgS5LWs9zTrPQYAAACccVM/o+Tm5qaSJUvqyJEjkv7+TB+bzebiqnA9jDE6d+6cjhw5opIlS8rNzc3VJQEAAOAGdFMHJUkKDQ2VJHtYQtFQsmRJ+3sLAAAAOOumD0o2m01hYWEKDg7WX3/95epykAfc3d0ZSQIAAMB1uemDUhY3Nzf+uAYAAAAg6SafzAEAAAAAckJQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWBCUAAAAAsCAoAQAAAIAFQQkAAAAALAhKAAAAAGBBUAIAAAAAC4ISAAAAAFgQlAAAAADAorirC0DhdXBEVVeX4BLlh+5wdQkAAABwMUaUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWBCUAAAAAsCAoAQAAAIAFQQkAAAAALIq7uoAbQa0XZ7i6BJeY7+fqCgAAAADXYEQJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIJZ7wAAAPLYwRFVXV2CS5QfusPVJQB5hhElAAAAALAgKAEAAACABUEJAAAAACwISgAAAABg4dKglJycrDp16sjPz0/BwcFq06aN9uzZ49AnMTFRNpvN4XXXXXe5qGIAAAAANwOXBqXVq1erZ8+e2rhxo5YuXaqLFy+qadOmOnv2rEO/5s2bKyUlxf766quvXFQxAAAAgJuBS6cHX7RokcPy1KlTFRwcrC1btuiee+6xt3t6eio0NLSgywMAAABwkypUzyidPn1akhQYGOjQvmrVKgUHB6tixYrq3r27jhw5csV9pKenKy0tzeEFAAAAAM4oNEHJGKO+ffvq7rvvVkxMjL29RYsW+uSTT7RixQqNHTtWmzZtUqNGjZSenp7jfpKTkxUQEGB/hYeHF9S3AAAAAKCIcOmtd5fr1auXtm/frnXr1jm0d+zY0f51TEyMateurYiICH355Zdq165dtv0MGjRIffv2tS+npaURlgAAAAA4pVAEpeeee06ff/651qxZo3Llyl21b1hYmCIiIvTzzz/nuN7T01Oenp75USYAAACAm4RLg5IxRs8995zmz5+vVatWKSoq6h+3OX78uA4dOqSwsLACqBAAAADAzcilzyj17NlTH3/8sWbOnCk/Pz+lpqYqNTVV58+flyT9+eef6t+/vzZs2KADBw5o1apVat26tUqXLq22bdu6snQAAAAARZhLR5QmTZokSYqPj3donzp1qhITE+Xm5qYdO3ZoxowZOnXqlMLCwpSQkKA5c+bIz8/PBRUDAAAAuBm4/Na7q/H29tbixYsLqBoAAAAA+FuhmMwBwN9qvTjD1SW4xJYxnV1dAgAAgINC8zlKAAAAAFBYEJQAAAAAwIJb7wDgJsRtngAAXB0jSgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwILJHAAAQL65WScOme/n6goAXC9GlAAAAADAgqAEAAAAABYEJQAAAACwICgBAAAAgAVBCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAo7uoCAAAAAFybgyOquroElyg/dEe+H4MRJQAAAACwICgBAAAAgAVBCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWBCUAAAAAsLjmoJSRkaE9e/bo4sWLeVkPAAAAALic00Hp3Llz6tatm3x8fFSlShUdPHhQkvT8889r9OjReV4gAAAAABQ0p4PSoEGDtG3bNq1atUpeXl729nvvvVdz5szJ0+IAAAAAwBWKO7vBggULNGfOHN11112y2Wz29sqVK2vfvn15WhwAAAAAuILTI0pHjx5VcHBwtvazZ886BCcAAAAAuFE5HZTq1KmjL7/80r6cFY4mT56suLi4vKsMAAAAAFzE6VvvkpOT1bx5c+3atUsXL17UhAkTtHPnTm3YsEGrV6/OjxoBAAAAoEA5PaJUr149rV+/XufOndNtt92mJUuWKCQkRBs2bFCtWrXyo0YAAAAAKFBOjSj99ddfeuqppzRkyBBNnz49v2oCAAAAAJdyakTJ3d1d8+fPz69aAAAAAKBQcPrWu7Zt22rBggX5UAoAAAAAFA5OT+Zw++2369VXX9X69etVq1YtlShRwmH9888/n2fFAQAAAIArOB2UPvzwQ5UsWVJbtmzRli1bHNbZbDaCEgAAAIAbntNBaf/+/flRBwAAAAAUGk4/o3Q5Y4yMMXlVCwAAAAAUCtcUlGbMmKGqVavK29tb3t7eio2N1UcffZTXtQEAAACASzh96924ceM0ZMgQ9erVS/Xr15cxRt98842efvppHTt2TH369MmPOgEAuG4HR1R1dQkuUX7oDleXAAA3HKeD0jvvvKNJkyapc+fO9rYHHnhAVapUUVJSEkEJAAAAwA3P6aCUkpKievXqZWuvV6+eUlJSnNpXcnKy5s2bpx9//FHe3t6qV6+eXn/9dVWqVMnexxij4cOH64MPPtDJkydVt25dvffee6pSpYqzpQMopPgvPwAAKGycfkbp9ttv19y5c7O1z5kzRxUqVHBqX6tXr1bPnj21ceNGLV26VBcvXlTTpk119uxZe5833nhD48aN07vvvqtNmzYpNDRUTZo00ZkzZ5wtHQAAAAByxekRpeHDh6tjx45as2aN6tevL5vNpnXr1mn58uU5BqirWbRokcPy1KlTFRwcrC1btuiee+6RMUbjx4/X4MGD1a5dO0nS9OnTFRISopkzZ6pHjx7Olg8AAAAA/8jpEaX27dvr22+/VenSpbVgwQLNmzdPpUuX1v/+9z+1bdv2uoo5ffq0JCkwMFDS35/ZlJqaqqZNm9r7eHp6qmHDhlq/fn2O+0hPT1daWprDCwAAAACc4fSIkiTVqlVLH3/8cZ4WYoxR3759dffddysmJkaSlJqaKkkKCQlx6BsSEqJff/01x/0kJydr+PDheVobAAAAgJuL0yNKX331lRYvXpytffHixfr666+vuZBevXpp+/btmjVrVrZ1NpvNYdkYk60ty6BBg3T69Gn769ChQ9dcEwAAAICbk9NBaeDAgbp06VK2dmOMBg4ceE1FPPfcc/r888+1cuVKlStXzt4eGhoq6f9HlrIcOXIk2yhTFk9PT/n7+zu8AAAAAMAZTgeln3/+WZUrV87Wfscdd2jv3r1O7csYo169emnevHlasWKFoqKiHNZHRUUpNDRUS5cutbdlZGRo9erVOU5RDgAAAAB5welnlAICAvTLL78oMjLSoX3v3r0qUaKEU/vq2bOnZs6cqf/+97/y8/OzjxwFBATI29tbNptNvXv31qhRo1ShQgVVqFBBo0aNko+Pjx599FFnSwcAAACAXHE6KN1///3q3bu35s+fr9tuu03S3yGpX79+uv/++53a16RJkyRJ8fHxDu1Tp05VYmKiJGnAgAE6f/68nn32WfsHzi5ZskR+fn7Olg4AAAAAueJ0UBozZoyaN2+uO+64w/480W+//aYGDRrozTffdGpfxph/7GOz2ZSUlKSkpCRnSwUAAACAa3JNt96tX79eS5cu1bZt2+Tt7a3Y2Fjdc889+VEfAAAAABS4a/ocJZvNpqZNmzp8ECwAAAAAFBW5nvXu22+/zfY5STNmzFBUVJSCg4P11FNPKT09Pc8LBAAAAICCluuglJSUpO3bt9uXd+zYoW7duunee+/VwIED9cUXXyg5OTlfigQAAACAgpTroLR161Y1btzYvjx79mzVrVtXkydPVt++ffX2229r7ty5+VIkAAAAABSkXAelkydPKiQkxL68evVqNW/e3L5cp04dHTp0KG+rAwAAAAAXyHVQCgkJ0f79+yVJGRkZ+u677xQXF2dff+bMGbm7u+d9hQAAAABQwHIdlJo3b66BAwdq7dq1GjRokHx8fNSgQQP7+u3bt9s/gBYAAAAAbmS5nh78tddeU7t27dSwYUP5+vpq+vTp8vDwsK+fMmUK04UDAAAAKBJyHZRuueUWrV27VqdPn5avr6/c3Nwc1n/66afy9fXN8wIBAAAAoKA5/YGzAQEBObYHBgZedzEAAAAAUBjk+hklAAAAALhZEJQAAAAAwIKgBAAAAAAWTgels2fP5kcdAAAAAFBoOB2UQkJC1LVrV61bty4/6gEAAAAAl3M6KM2aNUunT59W48aNVbFiRY0ePVqHDx/Oj9oAAAAAwCWcDkqtW7fWZ599psOHD+uZZ57RrFmzFBERoVatWmnevHm6ePFiftQJAAAAAAXmmidzCAoKUp8+fbRt2zaNGzdOy5Yt04MPPqgyZcpo6NChOnfuXF7WCQAAAAAFxukPnM2SmpqqGTNmaOrUqTp48KAefPBBdevWTYcPH9bo0aO1ceNGLVmyJC9rBQAAAIAC4XRQmjdvnqZOnarFixercuXK6tmzpx5//HGVLFnS3qd69eqqUaNGXtYJAAAAAAXG6aD0xBNP6OGHH9Y333yjOnXq5Njn1ltv1eDBg6+7OAAAAABwBaeDUkpKinx8fK7ax9vbW8OGDbvmogAAAADAlZyezMHPz09HjhzJ1n78+HG5ubnlSVEAAAAA4EpOByVjTI7t6enp8vDwuO6CAAAAAMDVcn3r3dtvvy1Jstls+vDDD+Xr62tfd+nSJa1Zs0Z33HFH3lcIAAAAAAUs10HprbfekvT3iNL777/vcJudh4eHIiMj9f777+d9hQAAAABQwHIdlPbv3y9JSkhI0Lx581SqVKl8KwoAAAAAXMnpWe9WrlyZH3UAAAAAQKGRq6DUt29fvfrqqypRooT69u171b7jxo3Lk8IAAAAAwFVyFZS+//57/fXXX/avr8Rms+VNVQAAAADgQrkKSpffbsetdwAAAACKOqc/R+mPP/644rrt27dfVzEAAAAAUBg4HZSqVq2qzz//PFv7m2++qbp16+ZJUQAAAADgSk4HpZdeekkdO3bU008/rfPnz+v3339Xo0aNNGbMGM2ZMyc/agQAAACAAuV0UOrXr582btyob775RrGxsYqNjZW3t7e2b9+u+++/Pz9qBAAAAIAC5XRQkqRbb71VVapU0YEDB5SWlqYOHTooJCQkr2sDAAAAAJdwOihljSTt3btX27dv16RJk/Tcc8+pQ4cOOnnyZH7UCAAAAAAFyumg1KhRI3Xs2FEbNmxQdHS0nnzySX3//ff67bffVLVq1fyoEQAAAAAKVK4+R+lyS5YsUcOGDR3abrvtNq1bt04jR47Ms8IAAAAAwFWcHlHKCkl79+7V4sWLdf78eUmSzWbTkCFD8rY6AAAAAHABp4PS8ePH1bhxY1WsWFEtW7ZUSkqKJOnJJ59U//7987xAAAAAAChoTgelPn36yN3dXQcPHpSPj4+9vWPHjvr666/ztDgAAAAAcIVrekZp8eLFKleunEN7hQoV9Ouvv+ZZYQAAAADgKk6PKJ09e9ZhJCnLsWPH5OnpmSdFAQAAAIArOR2U7rnnHs2YMcO+bLPZlJmZqTFjxighISFPiwMAAAAAV3D61rsxY8YoPj5emzdvVkZGhgYMGKCdO3fqxIkT+uabb/KjRgAAAAAoUE6PKFWuXFnbt2/XnXfeqSZNmujs2bNq166dvv/+e9122235USMAAAAAFCinR5QkKTQ0VMOHD8/rWgAAAACgUMhVUNq+fXuudxgbG3vNxQAAAABAYZCroFS9enXZbDYZY67az2az6dKlS3lSGAAAAAC4Sq6C0v79+/O7DgAAAAAoNHIVlCIiIvK7DgAAAAAoNK5pMoc9e/bonXfe0e7du2Wz2XTHHXfoueeeU6VKlfK6PgAAAAAocE5PD/6f//xHMTEx2rJli6pVq6bY2Fh99913iomJ0aeffpofNQIAAABAgXJ6RGnAgAEaNGiQRowY4dA+bNgwvfTSS3rooYfyrDgAAAAAcAWnR5RSU1PVuXPnbO2PP/64UlNTndrXmjVr1Lp1a5UpU0Y2m00LFixwWJ+YmCibzebwuuuuu5wtGQAAAACc4nRQio+P19q1a7O1r1u3Tg0aNHBqX2fPnlW1atX07rvvXrFP8+bNlZKSYn999dVXzpYMAAAAAE5x+ta7+++/Xy+99JK2bNliH93ZuHGjPv30Uw0fPlyff/65Q9+radGihVq0aHHVPp6engoNDXW2TAAAAAC4Zk4HpWeffVaSNHHiRE2cODHHdVLeffjsqlWrFBwcrJIlS6phw4YaOXKkgoODr9g/PT1d6enp9uW0tLTrrgEAAADAzcXpW+8yMzNz9cqLkNSiRQt98sknWrFihcaOHatNmzapUaNGDkHIKjk5WQEBAfZXeHj4ddcBAAAA4OZyTZ+jVFA6duxo/zomJka1a9dWRESEvvzyS7Vr1y7HbQYNGqS+ffval9PS0ghLAAAAAJxyTUHpf//7n1atWqUjR44oMzPTYd24cePypLCchIWFKSIiQj///PMV+3h6esrT0zPfagAAAABQ9DkdlEaNGqVXXnlFlSpVUkhIiGw2m33d5V/nh+PHj+vQoUMKCwvL1+MAAAAAuLk5HZQmTJigKVOmKDEx8boP/ueff2rv3r325f3792vr1q0KDAxUYGCgkpKS1L59e4WFhenAgQN6+eWXVbp0abVt2/a6jw0AAAAAV+J0UCpWrJjq16+fJwffvHmzEhIS7MtZzxZ16dJFkyZN0o4dOzRjxgydOnVKYWFhSkhI0Jw5c+Tn55cnxwcAAACAnDgdlPr06aP33ntP48ePv+6Dx8fHyxhzxfWLFy++7mMAAAAAgLOcDkr9+/fXfffdp9tuu02VK1eWu7u7w/p58+blWXEAAAAA4ApOB6XnnntOK1euVEJCgoKCgvJ9AgcAAAAAKGhOB6UZM2bos88+03333Zcf9QAAAACAyxVzdoPAwEDddttt+VELAAAAABQKTgelpKQkDRs2TOfOncuPegAAAADA5Zy+9e7tt9/Wvn37FBISosjIyGyTOXz33Xd5VhwAAAAAuILTQalNmzb5UAYAAAAAFB5OB6Vhw4blRx0AAAAAUGg4HZSybNmyRbt375bNZlPlypVVo0aNvKwLAAAAAFzG6aB05MgRPfzww1q1apVKliwpY4xOnz6thIQEzZ49W7fcckt+1AkAAAAABcbpWe+ee+45paWlaefOnTpx4oROnjypH374QWlpaXr++efzo0YAAAAAKFBOjygtWrRIy5YtU3R0tL2tcuXKeu+999S0adM8LQ4AAAAAXMHpEaXMzMxsU4JLkru7uzIzM/OkKAAAAABwJaeDUqNGjfTCCy/o8OHD9rbff/9dffr0UePGjfO0OAAAAABwBaeD0rvvvqszZ84oMjJSt912m26//XZFRUXpzJkzeuedd/KjRgAAAAAoUE4/oxQeHq7vvvtOS5cu1Y8//ihjjCpXrqx77703P+oDAAAAgAJ3zZ+j1KRJEzVp0iQvawEAAACAQiHXt96tWLFClStXVlpaWrZ1p0+fVpUqVbR27do8LQ4AAAAAXCHXQWn8+PHq3r27/P39s60LCAhQjx49NG7cuDwtDgAAAABcIddBadu2bWrevPkV1zdt2lRbtmzJk6IAAAAAwJVyHZT++OOPHD8/KUvx4sV19OjRPCkKAAAAAFwp10GpbNmy2rFjxxXXb9++XWFhYXlSFAAAAAC4Uq6DUsuWLTV06FBduHAh27rz589r2LBhatWqVZ4WBwAAAACukOvpwV955RXNmzdPFStWVK9evVSpUiXZbDbt3r1b7733ni5duqTBgwfnZ60AAAAAUCByHZRCQkK0fv16PfPMMxo0aJCMMZIkm82mZs2aaeLEiQoJCcm3QgEAAACgoDj1gbMRERH66quvdPLkSe3du1fGGFWoUEGlSpXKr/oAAAAAoMA5FZSylCpVSnXq1MnrWgAAAACgUMj1ZA4AAAAAcLMgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWBCUAAAAAsCAoAQAAAIAFQQkAAAAALAhKAAAAAGBBUAIAAAAAC4ISAAAAAFgQlAAAAADAgqAEAAAAABYEJQAAAACwICgBAAAAgAVBCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuXBqU1a9aodevWKlOmjGw2mxYsWOCw3hijpKQklSlTRt7e3oqPj9fOnTtdUywAAACAm4ZLg9LZs2dVrVo1vfvuuzmuf+ONNzRu3Di9++672rRpk0JDQ9WkSROdOXOmgCsFAAAAcDMp7sqDt2jRQi1atMhxnTFG48eP1+DBg9WuXTtJ0vTp0xUSEqKZM2eqR48eBVkqAAAAgJtIoX1Gaf/+/UpNTVXTpk3tbZ6enmrYsKHWr19/xe3S09OVlpbm8AIAAAAAZxTaoJSamipJCgkJcWgPCQmxr8tJcnKyAgIC7K/w8PB8rRMAAABA0VNog1IWm83msGyMydZ2uUGDBun06dP216FDh/K7RAAAAABFjEufUbqa0NBQSX+PLIWFhdnbjxw5km2U6XKenp7y9PTM9/oAAAAAFF2FdkQpKipKoaGhWrp0qb0tIyNDq1evVr169VxYGQAAAICizqUjSn/++af27t1rX96/f7+2bt2qwMBAlS9fXr1799aoUaNUoUIFVahQQaNGjZKPj48effRRF1YNAAAAoKhzaVDavHmzEhIS7Mt9+/aVJHXp0kXTpk3TgAEDdP78eT377LM6efKk6tatqyVLlsjPz89VJQMAAAC4Cbg0KMXHx8sYc8X1NptNSUlJSkpKKriiAAAAANz0Cu0zSgAAAADgKgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWBCUAAAAAsCAoAQAAAIAFQQkAAAAALAhKAAAAAGBBUAIAAAAAC4ISAAAAAFgQlAAAAADAgqAEAAAAABYEJQAAAACwICgBAAAAgAVBCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWBCUAAAAAsCAoAQAAAIAFQQkAAAAALAhKAAAAAGBBUAIAAAAAC4ISAAAAAFgQlAAAAADAgqAEAAAAABYEJQAAAACwICgBAAAAgAVBCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWhTooJSUlyWazObxCQ0NdXRYAAACAIq64qwv4J1WqVNGyZcvsy25ubi6sBgAAAMDNoNAHpeLFizs1ipSenq709HT7clpaWn6UBQAAAKAIK9S33knSzz//rDJlyigqKkoPP/ywfvnll6v2T05OVkBAgP0VHh5eQJUCAAAAKCoKdVCqW7euZsyYocWLF2vy5MlKTU1VvXr1dPz48StuM2jQIJ0+fdr+OnToUAFWDAAAAKAoKNS33rVo0cL+ddWqVRUXF6fbbrtN06dPV9++fXPcxtPTU56engVVIgAAAIAiqFCPKFmVKFFCVatW1c8//+zqUgAAAAAUYTdUUEpPT9fu3bsVFhbm6lIAAAAAFGGFOij1799fq1ev1v79+/Xtt9/qwQcfVFpamrp06eLq0gAAAAAUYYX6GaXffvtNjzzyiI4dO6ZbbrlFd911lzZu3KiIiAhXlwYAAACgCCvUQWn27NmuLgEAAADATahQ33oHAAAAAK5AUAIAAAAAC4ISAAAAAFgQlAAAAADAgqAEAAAAABYEJQAAAACwICgBAAAAgAVBCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWBCUAAAAAsCAoAQAAAIAFQQkAAAAALAhKAAAAAGBBUAIAAAAAC4ISAAAAAFgQlAAAAADAgqAEAAAAABYEJQAAAACwICgBAAAAgAVBCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWBCUAAAAAMCCoAQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBAAAAAAWBCUAAAAAsCAoAQAAAIAFQQkAAAAALAhKAAAAAGBBUAIAAAAAC4ISAAAAAFgQlAAAAADAgqAEAAAAABYEJQAAAACwICgBAAAAgAVBCQAAAAAsboigNHHiREVFRcnLy0u1atXS2rVrXV0SAAAAgCKs0AelOXPmqHfv3ho8eLC+//57NWjQQC1atNDBgwddXRoAAACAIqrQB6Vx48apW7duevLJJxUdHa3x48crPDxckyZNcnVpAAAAAIqo4q4u4GoyMjK0ZcsWDRw40KG9adOmWr9+fY7bpKenKz093b58+vRpSVJaWto113Ep/fw1b3sjO+N+ydUluMT1XCvXi2vt5sK1VvC41goe19rNhWut4HGtXdt2xph/7Fuog9KxY8d06dIlhYSEOLSHhIQoNTU1x22Sk5M1fPjwbO3h4eH5UmNRFuPqAlwlOcDVFdx0uNZQULjWUFC41lBQuNauzZkzZxQQcPV9FOqglMVmszksG2OytWUZNGiQ+vbta1/OzMzUiRMnFBQUdMVtkF1aWprCw8N16NAh+fv7u7ocFGFcaygoXGsoKFxrKChca84zxujMmTMqU6bMP/Yt1EGpdOnScnNzyzZ6dOTIkWyjTFk8PT3l6enp0FayZMn8KrHI8/f35wcPBYJrDQWFaw0FhWsNBYVrzTn/NJKUpVBP5uDh4aFatWpp6dKlDu1Lly5VvXr1XFQVAAAAgKKuUI8oSVLfvn3VqVMn1a5dW3Fxcfrggw908OBBPf30064uDQAAAEARVeiDUseOHXX8+HGNGDFCKSkpiomJ0VdffaWIiAhXl1akeXp6atiwYdluYwTyGtcaCgrXGgoK1xoKCtda/rKZ3MyNBwAAAAA3kUL9jBIAAAAAuAJBCQAAAAAsCEoAAAAAYEFQAgAAAAALghKymThxoqKiouTl5aVatWpp7dq1ri4JRdCaNWvUunVrlSlTRjabTQsWLHB1SSiCkpOTVadOHfn5+Sk4OFht2rTRnj17XF0WiqBJkyYpNjbW/sGfcXFx+vrrr11dFm4CycnJstls6t27t6tLKXIISnAwZ84c9e7dW4MHD9b333+vBg0aqEWLFjp48KCrS0MRc/bsWVWrVk3vvvuuq0tBEbZ69Wr17NlTGzdu1NKlS3Xx4kU1bdpUZ8+edXVpKGLKlSun0aNHa/Pmzdq8ebMaNWqkBx54QDt37nR1aSjCNm3apA8++ECxsbGuLqVIYnpwOKhbt65q1qypSZMm2duio6PVpk0bJScnu7AyFGU2m03z589XmzZtXF0KirijR48qODhYq1ev1j333OPqclDEBQYGasyYMerWrZurS0ER9Oeff6pmzZqaOHGiXnvtNVWvXl3jx493dVlFCiNKsMvIyNCWLVvUtGlTh/amTZtq/fr1LqoKAPLO6dOnJf39ByyQXy5duqTZs2fr7NmziouLc3U5KKJ69uyp++67T/fee6+rSymyiru6ABQex44d06VLlxQSEuLQHhISotTUVBdVBQB5wxijvn376u6771ZMTIyry0ERtGPHDsXFxenChQvy9fXV/PnzVblyZVeXhSJo9uzZ+u6777Rp0yZXl1KkEZSQjc1mc1g2xmRrA4AbTa9evbR9+3atW7fO1aWgiKpUqZK2bt2qU6dO6bPPPlOXLl20evVqwhLy1KFDh/TCCy9oyZIl8vLycnU5RRpBCXalS5eWm5tbttGjI0eOZBtlAoAbyXPPPafPP/9ca9asUbly5VxdDoooDw8P3X777ZKk2rVra9OmTZowYYL+9a9/ubgyFCVbtmzRkSNHVKtWLXvbpUuXtGbNGr377rtKT0+Xm5ubCyssOnhGCXYeHh6qVauWli5d6tC+dOlS1atXz0VVAcC1M8aoV69emjdvnlasWKGoqChXl4SbiDFG6enpri4DRUzjxo21Y8cObd261f6qXbu2HnvsMW3dupWQlIcYUYKDvn37qlOnTqpdu7bi4uL0wQcf6ODBg3r66addXRqKmD///FN79+61L+/fv19bt25VYGCgypcv78LKUJT07NlTM2fO1H//+1/5+fnZR8wDAgLk7e3t4upQlLz88stq0aKFwsPDdebMGc2ePVurVq3SokWLXF0aihg/P79sz1mWKFFCQUFBPH+ZxwhKcNCxY0cdP35cI0aMUEpKimJiYvTVV18pIiLC1aWhiNm8ebMSEhLsy3379pUkdenSRdOmTXNRVShqsj7qID4+3qF96tSpSkxMLPiCUGT98ccf6tSpk1JSUhQQEKDY2FgtWrRITZo0cXVpAK4Rn6MEAAAAABY8owQAAAAAFgQlAAAAALAgKAEAAACABUEJAAAAACwISgAAAABgQVACAAAAAAuCEgAAAABYEJQAAAAAwIKgBADAZeLj49W7d29XlwEAcDGCEgAgTyUmJspms2V7NW/e3NWlFZjIyEiNHz/eYTnrPHh7eysyMlIdOnTQihUrXFckAOCqCEoAgDzXvHlzpaSkOLxmzZrl6rJcasSIEUpJSdGePXs0Y8YMlSxZUvfee69Gjhzp6tIAADkgKAEA8pynp6dCQ0MdXqVKlZIkrVq1Sh4eHlq7dq29/9ixY1W6dGmlpKRIkhYtWqS7775bJUuWVFBQkFq1aqV9+/bZ+x84cEA2m01z585VgwYN5O3trTp16uinn37Spk2bVLt2bfn6+qp58+Y6evSofbvExES1adNGw4cPV3BwsPz9/dWjRw9lZGRc8XvJyMjQgAEDVLZsWZUoUUJ169bVqlWrnD4nfn5+Cg0NVfny5XXPPffogw8+0JAhQzR06FDt2bPH6f0BAPIXQQkAUKCyngHq1KmTTp8+rW3btmnw4MGaPHmywsLCJElnz55V3759tWnTJi1fvlzFihVT27ZtlZmZ6bCvYcOG6ZVXXtF3332n4sWL65FHHtGAAQM0YcIErV27Vvv27dPQoUMdtlm+fLl2796tlStXatasWZo/f76GDx9+xXqfeOIJffPNN5o9e7a2b9+uhx56SM2bN9fPP/983efihRdekDFG//3vf697XwCAvFXc1QUAAIqehQsXytfX16HtpZde0pAhQyRJr732mpYtW6annnpKO3fuVKdOndS2bVt73/bt2zts++9//1vBwcHatWuXYmJi7O39+/dXs2bNJP0dOh555BEtX75c9evXlyR169ZN06ZNc9iXh4eHpkyZIh8fH1WpUkUjRozQiy++qFdffVXFijn+/3Dfvn2aNWuWfvvtN5UpU8Z+zEWLFmnq1KkaNWrUdZwlKTAwUMHBwTpw4MB17QcAkPcISgCAPJeQkKBJkyY5tAUGBtq/9vDw0Mcff6zY2FhFREQ4THwg/R1QhgwZoo0bN+rYsWP2kaSDBw86BKXY2Fj71yEhIZKkqlWrOrQdOXLEYd/VqlWTj4+PfTkuLk5//vmnDh06pIiICIe+3333nYwxqlixokN7enq6goKC/vE85IYxRjabLU/2BQDIOwQlAECeK1GihG6//far9lm/fr0k6cSJEzpx4oRKlChhX9e6dWuFh4dr8uTJKlOmjDIzMxUTE5PtWSJ3d3f711lhw9pmvV3vSnIKK5mZmXJzc9OWLVvk5ubmsM46YnYtjh8/rqNHjyoqKuq69wUAyFsEJQBAgdu3b5/69OmjyZMna+7cuercubP9WaTjx49r9+7d+te//qUGDRpIktatW5dnx962bZvOnz8vb29vSdLGjRvl6+urcuXKZetbo0YNXbp0SUeOHLHXkpcmTJigYsWKqU2bNnm+bwDA9SEoAQDyXHp6ulJTUx3aihcvrtKlS+vSpUvq1KmTmjZtqieeeEItWrRQ1apVNXbsWL344osqVaqUgoKC9MEHHygsLEwHDx7UwIED86y2jIwMdevWTa+88op+/fVXDRs2TL169cr2fJIkVaxYUY899pg6d+6ssWPHqkaNGjp27JhWrFihqlWrqmXLlrk+7pkzZ5Samqq//vpL+/fv18cff6wPP/xQycnJ/zj6BgAoeAQlAECeW7RokX0GuyyVKlXSjz/+qJEjR+rAgQP64osvJEmhoaH68MMP1aFDBzVp0kTVq1fX7Nmz9fzzzysmJkaVKlXS22+/rfj4+DyprXHjxqpQoYLuuecepaen6+GHH1ZSUtIV+0+dOlWvvfaa+vXrp99//11BQUGKi4tzKiRJ0tChQzV06FB5eHgoNDRUd911l5YvX66EhITr/I4AAPnBZowxri4CAICCkJiYqFOnTmnBggWuLgUAUMjxOUoAAAAAYEFQAgAAAAALbr0DAAAAAAtGlAAAAADAgqAEAAAAABYEJQAAAACwICgBAAAAgAVBCQAAAAAsCEoAAAAAYEFQAgAAAAALghIAAAAAWPwfYn+kvSVaQCIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Analyze Examples for Complexity\n",
    "# Function to analyze complexity of text with word-by-word breakdown\n",
    "def analyze_complexity(text):\n",
    "    # Get sentence complexity\n",
    "    sent_complexity = sentence_complexity_score(text)\n",
    "    \n",
    "    # Get word complexities\n",
    "    words = word_tokenize(text)\n",
    "    word_complexities = []\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) > 2 and word.isalpha():\n",
    "            complexity = word_complexity_score(word)\n",
    "            word_complexities.append((word, complexity))\n",
    "    \n",
    "    # Sort by complexity for better analysis\n",
    "    word_complexities.sort(key=lambda x: x[1], reverse=True)\n",
    "    avg_word_complexity = np.mean([c for _, c in word_complexities]) if word_complexities else 0\n",
    "    max_word_complexity = np.max([c for _, c in word_complexities]) if word_complexities else 0\n",
    "    \n",
    "    # Find the most complex words\n",
    "    most_complex_words = word_complexities[:5]\n",
    "    \n",
    "    return {\n",
    "        'sentence_complexity': sent_complexity,\n",
    "        'avg_word_complexity': avg_word_complexity,\n",
    "        'max_word_complexity': max_word_complexity,\n",
    "        'most_complex_words': most_complex_words,\n",
    "        'text_length': len(text),\n",
    "        'word_count': len(words),\n",
    "        'word_complexities': word_complexities  # Added for word-by-word analysis\n",
    "    }\n",
    "\n",
    "# Analyze a few examples with enhanced output\n",
    "num_examples = 5\n",
    "results = []\n",
    "\n",
    "# Properly access the dataset by index\n",
    "validation_data = asset[\"validation\"]\n",
    "for i in range(num_examples):\n",
    "    # Get example by index\n",
    "    example = validation_data[i]\n",
    "    \n",
    "    original = example['original']\n",
    "    original_analysis = analyze_complexity(original)\n",
    "    original_analysis['text_type'] = 'Original'\n",
    "    original_analysis['example_id'] = i\n",
    "    original_analysis['text'] = original\n",
    "    results.append(original_analysis)\n",
    "    \n",
    "    # Analyze first simplification\n",
    "    simplification = example['simplifications'][0]\n",
    "    simp_analysis = analyze_complexity(simplification)\n",
    "    simp_analysis['text_type'] = 'Simplified'\n",
    "    simp_analysis['example_id'] = i\n",
    "    simp_analysis['text'] = simplification\n",
    "    results.append(simp_analysis)\n",
    "\n",
    "# Convert results to dataframe for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "print(results_df[['example_id', 'text_type', 'text', 'sentence_complexity', 'avg_word_complexity']])\n",
    "\n",
    "# Calculate average complexity reduction\n",
    "original_avg = results_df[results_df['text_type'] == 'Original']['sentence_complexity'].mean()\n",
    "simplified_avg = results_df[results_df['text_type'] == 'Simplified']['sentence_complexity'].mean()\n",
    "reduction = original_avg - simplified_avg\n",
    "percent_reduction = (reduction / original_avg) * 100\n",
    "print(f\"\\nAverage sentence complexity reduction: {reduction:.2f} points ({percent_reduction:.2f}%)\")\n",
    "\n",
    "# Print word-by-word complexity breakdown for first example\n",
    "print(\"\\nWord-by-word complexity breakdown for first example:\")\n",
    "original_words = results[0]['word_complexities']\n",
    "print(\"Original text:\")\n",
    "print(results[0]['text'])\n",
    "print(\"\\nComplex words (complexity > 50):\")\n",
    "for word, complexity in original_words:\n",
    "    if complexity > 50:\n",
    "        print(f\"  - '{word}': {complexity:.1f}\")\n",
    "\n",
    "# Plot the complexity comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='example_id', y='sentence_complexity', hue='text_type', data=results_df)\n",
    "plt.title('Sentence Complexity: Original vs Simplified')\n",
    "plt.xlabel('Example ID')\n",
    "plt.ylabel('Complexity Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3:\n",
      "Original: They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", introducing two new variants of the race; the Cyber-Shades and the Cyber-King.\n",
      "Complex words (0):\n",
      "Complex sentences (1):\n",
      "  - 'They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", introducing two new variants of the race; the Cyber-Shades and the Cyber-King.': 101.0\n",
      "\n",
      "Detailed word breakdown:\n",
      "  - 'They': 11.0 (1 syllables)\n",
      "  - 'would': 13.0 (1 syllables)\n",
      "  - 'later': 16.0 (2 syllables)\n",
      "  - 'return': 18.0 (2 syllables)\n",
      "  - 'the': 9.0 (1 syllables)\n",
      "  - 'revived': 20.0 (2 syllables)\n",
      "  - 'series': 18.0 (2 syllables)\n",
      "  - 'the': 9.0 (1 syllables)\n",
      "  - 'Christmas': 24.0 (2 syllables)\n",
      "  - 'Special': 20.0 (2 syllables)\n",
      "  - 'The': 9.0 (1 syllables)\n",
      "  - 'Next': 11.0 (1 syllables)\n",
      "  - 'Doctor': 18.0 (2 syllables)\n",
      "  - 'introducing': 34.0 (4 syllables)\n",
      "  - 'two': 9.0 (1 syllables)\n",
      "  - 'new': 9.0 (1 syllables)\n",
      "  - 'variants': 25.0 (3 syllables)\n",
      "  - 'the': 9.0 (1 syllables)\n",
      "  - 'race': 11.0 (1 syllables)\n",
      "  - 'the': 9.0 (1 syllables)\n",
      "  - 'and': 9.0 (1 syllables)\n",
      "  - 'the': 9.0 (1 syllables)\n",
      "\n",
      "Example 4:\n",
      "Original: Jameson's autobiography, How to Make Love Like a Porn Star: A Cautionary Tale was published August 17, 2004.\n",
      "Complex words (0):\n",
      "Complex sentences (1):\n",
      "  - 'Jameson's autobiography, How to Make Love Like a Porn Star: A Cautionary Tale was published August 17, 2004.': 71.1\n",
      "\n",
      "Example 5:\n",
      "Original: It is particularly famous for the cultivation of kiwifruit.\n",
      "Complex words (0):\n",
      "Complex sentences (0):\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "try:\n",
    "    cmudict.ensure_loaded()\n",
    "except LookupError:\n",
    "    nltk.download('cmudict')\n",
    "\n",
    "try:\n",
    "    word_tokenize(\"example\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    sent_tokenize(\"An example sentence.\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "d = cmudict.dict()\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"\n",
    "    Counts the number of syllables in a word using the CMU Pronouncing Dictionary.\n",
    "    Returns the syllable count or 0 if the word is not found.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word in d:\n",
    "        # Return the number of syllables in the first pronunciation\n",
    "        return len([phoneme for phoneme in d[word][0] if phoneme[-1].isdigit()])\n",
    "    else:\n",
    "        # Fallback for words not in the dictionary (simplified vowel counting)\n",
    "        vowels = \"aeiouy\"\n",
    "        count = 0\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"e\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        return count\n",
    "\n",
    "def word_complexity_score(word):\n",
    "    \"\"\"\n",
    "    Calculates a complexity score for a word based on its length and syllable count.\n",
    "    More syllables and longer words are considered more complex.\n",
    "    \"\"\"\n",
    "    syllables = count_syllables(word)\n",
    "    length_factor = len(word) * 2  # Emphasize length\n",
    "    syllable_factor = syllables * 3  # Emphasize syllables more\n",
    "    return length_factor + syllable_factor\n",
    "\n",
    "def sentence_complexity_score(sentence):\n",
    "    \"\"\"\n",
    "    Calculates a complexity score for a sentence based on the average word complexity\n",
    "    and the sentence length. Longer sentences with more complex words have a higher score.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    if not words:\n",
    "        return 0\n",
    "    valid_words = [word for word in words if word.isalpha() and len(word) > 2]\n",
    "    if not valid_words:\n",
    "        return len(sentence)  # Consider length if no complex words\n",
    "    total_complexity = sum(word_complexity_score(word) for word in valid_words)\n",
    "    average_complexity = total_complexity / len(valid_words)\n",
    "    sentence_length_factor = len(sentence) * 0.5\n",
    "    return average_complexity + sentence_length_factor\n",
    "\n",
    "def identify_complex_words(text, threshold=55):\n",
    "    \"\"\"\n",
    "    Identify words that exceed the complexity threshold\n",
    "    Returns a list of (word, complexity_score) tuples for complex words\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    complex_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(word) > 2 and word.isalpha():  # Skip short words and non-alphabetic tokens\n",
    "            complexity = word_complexity_score(word)\n",
    "            if complexity > threshold:\n",
    "                complex_words.append((word, complexity))\n",
    "\n",
    "    # Sort by complexity (descending)\n",
    "    complex_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    return complex_words\n",
    "\n",
    "def identify_complex_sentences(text, threshold=65):\n",
    "    \"\"\"\n",
    "    Identify sentences that exceed the complexity threshold\n",
    "    Returns a list of (sentence, complexity_score) tuples for complex sentences\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    complex_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        complexity = sentence_complexity_score(sentence)\n",
    "        if complexity > threshold:\n",
    "            complex_sentences.append((sentence, complexity))\n",
    "\n",
    "    # Sort by complexity (descending)\n",
    "    complex_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "    return complex_sentences\n",
    "\n",
    "def analyze_word_breakdown(text):\n",
    "    \"\"\"Detailed analysis of each word in the text\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    results = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(word) > 2 and word.isalpha():\n",
    "            complexity = word_complexity_score(word)\n",
    "            syllables = count_syllables(word)\n",
    "            results.append({\n",
    "                'word': word,\n",
    "                'complexity': complexity,\n",
    "                'syllables': syllables,\n",
    "                'is_complex': complexity > 55\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "validation_data = asset[\"validation\"]\n",
    "\n",
    "for i in range(2,5):  # Get examples 3-5\n",
    "    example = validation_data[i]  # Get by index\n",
    "\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {example['original']}\")\n",
    "\n",
    "    # Identify complex words with lower threshold\n",
    "    complex_words = identify_complex_words(example['original'])\n",
    "    print(f\"Complex words ({len(complex_words)}):\")\n",
    "    for word, score in complex_words[:5]:  # Show top 5\n",
    "        print(f\"  - '{word}': {score:.1f}\")\n",
    "\n",
    "    # Identify complex sentences with lower threshold\n",
    "    complex_sentences = identify_complex_sentences(example['original'])\n",
    "    print(f\"Complex sentences ({len(complex_sentences)}):\")\n",
    "    for sentence, score in complex_sentences:\n",
    "        print(f\"  - '{sentence}': {score:.1f}\")\n",
    "\n",
    "    # Print word-by-word breakdown for the first example\n",
    "    if i == 2:\n",
    "        print(\"\\nDetailed word breakdown:\")\n",
    "        breakdown = analyze_word_breakdown(example['original'])\n",
    "        for word_info in breakdown:\n",
    "            if word_info['is_complex']:\n",
    "                print(f\"  - '{word_info['word']}': {word_info['complexity']:.1f} (complex, {word_info['syllables']} syllables)\")\n",
    "            elif word_info['complexity'] > 0:\n",
    "                print(f\"  - '{word_info['word']}': {word_info['complexity']:.1f} ({word_info['syllables']} syllables)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved word substitution simplification:\n",
      "\n",
      "Example 1:\n",
      "Original: The ubiquitous nature of smartphones has revolutionized modern communication.\n",
      "Word complexities:\n",
      "\n",
      "No substitutions found at any threshold.\n",
      "\n",
      "Example 2:\n",
      "Original: The protagonist's epiphany led to a profound transformation in his worldview.\n",
      "Word complexities:\n",
      "\n",
      "No substitutions found at any threshold.\n",
      "\n",
      "Example 3:\n",
      "Original: The government implemented austerity measures to address the fiscal deficit.\n",
      "Word complexities:\n",
      "\n",
      "No substitutions found at any threshold.\n",
      "\n",
      "Example 4:\n",
      "Original: This is the quintessential understanding for text simplification\n",
      "Word complexities:\n",
      "\n",
      "No substitutions found at any threshold.\n",
      "\n",
      "Example 5:\n",
      "Original: This was an absolutely smashing success and I wish for nothing but the best for you two\n",
      "Word complexities:\n",
      "\n",
      "No substitutions found at any threshold.\n"
     ]
    }
   ],
   "source": [
    "# 5. Simple Substitution-Based Simplification\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a word using WordNet with improved filtering\"\"\"\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name()\n",
    "            # Filter out multi-word expressions and weird forms\n",
    "            if '_' not in synonym and '-' not in synonym and synonym.isalpha():\n",
    "                synonyms.append(synonym)\n",
    "    return list(set(synonyms))  # Remove duplicates\n",
    "\n",
    "def find_simpler_synonym(word, original_complexity, verbose=False):\n",
    "    \"\"\"Find a simpler synonym for a word with better logging\"\"\"\n",
    "    synonyms = get_synonyms(word)\n",
    "    simpler_options = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Finding simpler synonyms for '{word}' (complexity: {original_complexity:.1f})\")\n",
    "    \n",
    "    for synonym in synonyms:\n",
    "        # Skip multi-word synonyms (connected by underscore)\n",
    "        if '_' in synonym or not synonym.isalpha():\n",
    "            continue\n",
    "            \n",
    "        # Calculate complexity of the synonym\n",
    "        synonym_complexity = word_complexity_score(synonym)\n",
    "        \n",
    "        # Only consider synonyms that are actually simpler\n",
    "        if synonym_complexity < original_complexity:\n",
    "            simpler_options.append((synonym, synonym_complexity))\n",
    "            if verbose:\n",
    "                print(f\"  - Option: '{synonym}' (complexity: {synonym_complexity:.1f})\")\n",
    "    \n",
    "    # Sort by complexity (ascending) and return the simplest option\n",
    "    if simpler_options:\n",
    "        simpler_options.sort(key=lambda x: x[1])\n",
    "        if verbose:\n",
    "            print(f\"  → Selected: '{simpler_options[0][0]}' (complexity: {simpler_options[0][1]:.1f})\")\n",
    "        return simpler_options[0]\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"  → No simpler options found\")\n",
    "        return None\n",
    "\n",
    "def simplify_text_by_word_substitution(text, complexity_threshold=55, verbose=False):\n",
    "    \"\"\"Simplify text by substituting complex words with simpler synonyms\"\"\"\n",
    "    complex_words = identify_complex_words(text, threshold=complexity_threshold)\n",
    "    \n",
    "    if not complex_words:\n",
    "        return text, []\n",
    "    \n",
    "    substitutions = []\n",
    "    simplified_text = text\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Found {len(complex_words)} complex words above threshold {complexity_threshold}\")\n",
    "    \n",
    "    for word, complexity in complex_words:\n",
    "        simpler_option = find_simpler_synonym(word, complexity, verbose=verbose)\n",
    "        if simpler_option:\n",
    "            simpler_word, simpler_complexity = simpler_option\n",
    "            # Make the substitution\n",
    "            simplified_text = re.sub(r'\\b' + re.escape(word) + r'\\b', simpler_word, simplified_text)\n",
    "            substitutions.append((word, simpler_word, complexity, simpler_complexity))\n",
    "    \n",
    "    # Check the complexity before and after\n",
    "    original_complexity = sentence_complexity_score(text)\n",
    "    new_complexity = sentence_complexity_score(simplified_text)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Text complexity: {original_complexity:.1f} → {new_complexity:.1f}\")\n",
    "    \n",
    "    return simplified_text, substitutions\n",
    "\n",
    "# Test the improved word substitution simplification on examples\n",
    "print(\"Testing improved word substitution simplification:\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"The ubiquitous nature of smartphones has revolutionized modern communication.\",\n",
    "    \"The protagonist's epiphany led to a profound transformation in his worldview.\",\n",
    "    \"The government implemented austerity measures to address the fiscal deficit.\",\n",
    "    \"This is the quintessential understanding for text simplification\",\n",
    "    \"This was an absolutely smashing success and I wish for nothing but the best for you two\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {sentence}\")\n",
    "    \n",
    "    # First show the word complexities\n",
    "    complex_words = identify_complex_words(sentence, threshold=50)\n",
    "    print(\"Word complexities:\")\n",
    "    for word, score in sorted(complex_words, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - '{word}': {score:.1f}\")\n",
    "    \n",
    "    # Try different thresholds\n",
    "    for threshold in [55, 50, 45]:\n",
    "        # Simplify the text\n",
    "        simplified, substitutions = simplify_text_by_word_substitution(\n",
    "            sentence, \n",
    "            complexity_threshold=threshold,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        if substitutions:\n",
    "            print(f\"\\nUsing threshold {threshold}:\")\n",
    "            print(f\"Simplified: {simplified}\")\n",
    "            print(f\"Word substitutions:\")\n",
    "            for orig, simp, orig_score, simp_score in substitutions:\n",
    "                print(f\"  - '{orig}' → '{simp}' (complexity: {orig_score:.1f} → {simp_score:.1f})\")\n",
    "            \n",
    "            # Calculate overall complexity reduction\n",
    "            orig_complexity = sentence_complexity_score(sentence)\n",
    "            simp_complexity = sentence_complexity_score(simplified)\n",
    "            reduction = orig_complexity - simp_complexity\n",
    "            \n",
    "            print(f\"Overall sentence complexity: {orig_complexity:.1f} → {simp_complexity:.1f} ({reduction:.1f} reduction)\")\n",
    "            break  # Only show one successful threshold\n",
    "        elif threshold == 45:  # Last threshold and still no substitutions\n",
    "            print(\"\\nNo substitutions found at any threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved semantic similarity function:\n",
      "\n",
      "Pair 1:\n",
      "Text 1: The company announced a new product yesterday.\n",
      "Text 2: The company announced a new product yesterday.\n",
      "Similarity score: 1.0000\n",
      "Interpretation: Very high\n",
      "\n",
      "Pair 2:\n",
      "Text 1: The company announced a new product yesterday.\n",
      "Text 2: The business revealed a new item yesterday.\n",
      "Similarity score: 0.5833\n",
      "Interpretation: Medium\n",
      "\n",
      "Pair 3:\n",
      "Text 1: The company announced a new product yesterday, which will be available next month.\n",
      "Text 2: The company announced a new product yesterday.\n",
      "Similarity score: 0.6352\n",
      "Interpretation: Medium\n",
      "\n",
      "Pair 4:\n",
      "Text 1: The company announced a new product yesterday.\n",
      "Text 2: A new product will be available from the company soon.\n",
      "Similarity score: 0.6600\n",
      "Interpretation: Medium\n",
      "\n",
      "Pair 5:\n",
      "Text 1: The company announced a new product yesterday.\n",
      "Text 2: The weather was pleasant throughout the weekend.\n",
      "Similarity score: 0.3864\n",
      "Interpretation: Low\n",
      "\n",
      "Comparing with human simplifications from the dataset:\n",
      "\n",
      "Example 1:\n",
      "Original: Adjacent counties are Marin (to the south), Mendocino (to the north), Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).\n",
      "Human simplification: countries next to it are Marin, Mendocino, Lake, Napa, Solano, and Contra Costa.\n",
      "Similarity score: 0.5958\n",
      "\n",
      "Example 2:\n",
      "Original: A Georgian inscription around the drum attests his name.\n",
      "Human simplification: A writing around the drum confirms his name.\n",
      "Similarity score: 0.6775\n",
      "\n",
      "Example 3:\n",
      "Original: They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", introducing two new variants of the race; the Cyber-Shades and the Cyber-King.\n",
      "Human simplification: They would return to the new series in the 2008 Special \"The Next Doctor\". There were two new variants; Cyber-Shades and the Cyber-King.\n",
      "Similarity score: 0.7492\n"
     ]
    }
   ],
   "source": [
    "# 6. Improved Meaning Preservation Evaluation\n",
    "def calculate_semantic_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between two texts using an enhanced approach.\n",
    "    \n",
    "    This function combines word overlap with other features to better estimate \n",
    "    meaning preservation between the original and simplified text.\n",
    "    \n",
    "    Args:\n",
    "        text1: First text (usually the original)\n",
    "        text2: Second text (usually the simplified version)\n",
    "        \n",
    "    Returns:\n",
    "        A similarity score between 0 and 1\n",
    "    \"\"\"\n",
    "    # Handle empty texts\n",
    "    if not text1 or not text2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Tokenize texts\n",
    "    words1 = word_tokenize(text1.lower())\n",
    "    words2 = word_tokenize(text2.lower())\n",
    "    \n",
    "    # Convert to sets of content words (exclude punctuation and stopwords)\n",
    "    content_words1 = set(word for word in words1 if word.isalpha() and len(word) > 2)\n",
    "    content_words2 = set(word for word in words2 if word.isalpha() and len(word) > 2)\n",
    "    \n",
    "    if not content_words1 or not content_words2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate Jaccard similarity (intersection over union)\n",
    "    intersection = len(content_words1.intersection(content_words2))\n",
    "    union = len(content_words1.union(content_words2))\n",
    "    jaccard = intersection / max(1, union)\n",
    "    \n",
    "    # Calculate word overlap as a proportion of the original words\n",
    "    overlap = intersection / len(content_words1)\n",
    "    \n",
    "    # Calculate length ratio (penalize if lengths are very different)\n",
    "    len_ratio = min(len(words2) / max(1, len(words1)), 1.0)\n",
    "    \n",
    "    # Calculate key word preservation (words specific to this text)\n",
    "    key_words1 = [word for word in content_words1 if word_complexity_score(word) > 40]\n",
    "    key_words2 = [word for word in content_words2 if word_complexity_score(word) > 40]\n",
    "    \n",
    "    key_preserved = sum(1 for word in key_words1 if word in key_words2)\n",
    "    key_preservation = key_preserved / max(1, len(key_words1)) if key_words1 else 1.0\n",
    "    \n",
    "    # Combine metrics with weights\n",
    "    similarity = (0.4 * jaccard) + (0.3 * overlap) + (0.1 * len_ratio) + (0.2 * key_preservation)\n",
    "    \n",
    "    # Ensure the result is between 0 and 1\n",
    "    return max(0.0, min(1.0, similarity))\n",
    "\n",
    "# Test the improved semantic similarity function\n",
    "print(\"Testing improved semantic similarity function:\")\n",
    "\n",
    "test_pairs = [\n",
    "    # Original text and identical text (perfect similarity)\n",
    "    (\"The company announced a new product yesterday.\", \n",
    "     \"The company announced a new product yesterday.\"),\n",
    "    \n",
    "    # Original text and simplification with synonyms (high similarity)\n",
    "    (\"The company announced a new product yesterday.\", \n",
    "     \"The business revealed a new item yesterday.\"),\n",
    "    \n",
    "    # Original text and text with deleted information (medium similarity)\n",
    "    (\"The company announced a new product yesterday, which will be available next month.\", \n",
    "     \"The company announced a new product yesterday.\"),\n",
    "    \n",
    "    # Original text and different but related text (low similarity)\n",
    "    (\"The company announced a new product yesterday.\", \n",
    "     \"A new product will be available from the company soon.\"),\n",
    "    \n",
    "    # Original text and completely different text (very low similarity)\n",
    "    (\"The company announced a new product yesterday.\", \n",
    "     \"The weather was pleasant throughout the weekend.\")\n",
    "]\n",
    "\n",
    "for i, (text1, text2) in enumerate(test_pairs):\n",
    "    similarity = calculate_semantic_similarity(text1, text2)\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"Text 1: {text1}\")\n",
    "    print(f\"Text 2: {text2}\")\n",
    "    print(f\"Similarity score: {similarity:.4f}\")\n",
    "    print(f\"Interpretation: \" + \n",
    "          (\"Very high\" if similarity > 0.9 else\n",
    "           \"High\" if similarity > 0.7 else\n",
    "           \"Medium\" if similarity > 0.5 else\n",
    "           \"Low\" if similarity > 0.3 else\n",
    "           \"Very low\"))\n",
    "\n",
    "# Compare with human simplifications from the dataset\n",
    "print(\"\\nComparing with human simplifications from the dataset:\")\n",
    "\n",
    "validation_data = asset[\"validation\"]\n",
    "for i in range(3):  # First 3 examples\n",
    "    example = validation_data[i]\n",
    "    original = example['original']\n",
    "    human_simplification = example['simplifications'][0]\n",
    "    \n",
    "    similarity = calculate_semantic_similarity(original, human_simplification)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Human simplification: {human_simplification}\")\n",
    "    print(f\"Similarity score: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing deletion-based simplification:\n",
      "\n",
      "Example 1:\n",
      "Original: Adjacent counties are Marin (to the south), Mendocino (to the north), Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).\n",
      "Simplified: Adjacent counties are Marin , and Solano and Contra Costa.\n",
      "Deleted content:\n",
      "  - '(to the southeast)' (parenthetical)\n",
      "  - ', Napa (to the east)' (appositive)\n",
      "  - ', Lake (northeast)' (appositive)\n",
      "  - ', Mendocino (to the north)' (appositive)\n",
      "  - '(to the south)' (parenthetical)\n",
      "Complexity: 92.8 → 45.1 (47.7 reduction)\n",
      "Human simplification: countries next to it are Marin, Mendocino, Lake, Napa, Solano, and Contra Costa.\n",
      "\n",
      "Example 2:\n",
      "Original: A Georgian inscription around the drum attests his name.\n",
      "Simplified: A Georgian inscription around the drum attests his name.\n",
      "Complexity: 44.4 → 44.4 (0.0 reduction)\n",
      "Human simplification: A writing around the drum confirms his name.\n",
      "\n",
      "Example 3:\n",
      "Original: They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", introducing two new variants of the race; the Cyber-Shades and the Cyber-King.\n",
      "Simplified: They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", introducing two new variants of the race; the Cyber-Shades and the Cyber-King.\n",
      "Complexity: 101.0 → 101.0 (0.0 reduction)\n",
      "Human simplification: They would return to the new series in the 2008 Special \"The Next Doctor\". There were two new variants; Cyber-Shades and the Cyber-King.\n",
      "\n",
      "Example 4:\n",
      "Original: Jameson's autobiography, How to Make Love Like a Porn Star: A Cautionary Tale was published August 17, 2004.\n",
      "Simplified: Jameson's autobiography, How to Make Love Like a Porn Star: A Cautionary Tale was published August 17, 2004.\n",
      "Complexity: 71.1 → 71.1 (0.0 reduction)\n",
      "Human simplification: Jameson's autobiography was published on August 17, 2004\n",
      "\n",
      "Example 5:\n",
      "Original: It is particularly famous for the cultivation of kiwifruit.\n",
      "Simplified: It is particularly famous for the cultivation of kiwifruit.\n",
      "Complexity: 52.2 → 52.2 (0.0 reduction)\n",
      "Human simplification: It is famous for the cultivation of kiwi fruit.\n"
     ]
    }
   ],
   "source": [
    "# 8. Deletion-Based Simplification\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "\n",
    "def identify_non_essential_phrases(text):\n",
    "    \"\"\"Identify phrases that might be non-essential for the core meaning\"\"\"\n",
    "    # Look for common patterns of non-essential information:\n",
    "    # 1. Appositive phrases (often between commas)\n",
    "    # 2. Parenthetical expressions\n",
    "    # 3. Some prepositional phrases\n",
    "    # 4. Non-restrictive relative clauses\n",
    "    \n",
    "    non_essential_spans = []\n",
    "    \n",
    "    # Find text in parentheses\n",
    "    for match in re.finditer(r'\\([^)]*\\)', text):\n",
    "        non_essential_spans.append((match.start(), match.end(), 'parenthetical'))\n",
    "    \n",
    "    # Find potential appositive phrases (text between commas)\n",
    "    # This is a simplified approach - a full implementation would use parsing\n",
    "    comma_positions = [m.start() for m in re.finditer(r',', text)]\n",
    "    \n",
    "    for i in range(len(comma_positions) - 1):\n",
    "        start = comma_positions[i]\n",
    "        end = comma_positions[i + 1]\n",
    "        \n",
    "        # Check if the span is short enough to likely be an appositive\n",
    "        if end - start < 40 and end - start > 5:\n",
    "            # Check if the text contains a noun phrase\n",
    "            phrase = text[start+1:end].strip()\n",
    "            tokens = word_tokenize(phrase)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            \n",
    "            # If the phrase has a noun and is not just a single word or conjunction\n",
    "            if any(tag.startswith('NN') for _, tag in pos_tags) and len(tokens) > 1:\n",
    "                # Make sure we're not in the middle of a list (shouldn't have 'and' or 'or')\n",
    "                if not any(word.lower() in ('and', 'or') for word in tokens):\n",
    "                    non_essential_spans.append((start, end, 'appositive'))\n",
    "    \n",
    "    # Find non-restrictive relative clauses (often starting with 'which' or 'who' after a comma)\n",
    "    for match in re.finditer(r',\\s+(which|who|whom|whose)', text, re.IGNORECASE):\n",
    "        start = match.start()\n",
    "        # Find the next comma or period\n",
    "        next_punct = next((text.find(p, start + 1) for p in [',', '.', ';'] \n",
    "                          if text.find(p, start + 1) != -1), len(text))\n",
    "        if next_punct - start > 5:\n",
    "            non_essential_spans.append((start, next_punct, 'non-restrictive'))\n",
    "    \n",
    "    # Remove overlapping spans\n",
    "    non_essential_spans.sort()\n",
    "    filtered_spans = []\n",
    "    for span in non_essential_spans:\n",
    "        # Check if this span overlaps with any spans we've already decided to keep\n",
    "        if not any(s <= span[0] < e or s < span[1] <= e for s, e, _ in filtered_spans):\n",
    "            filtered_spans.append(span)\n",
    "    \n",
    "    return filtered_spans\n",
    "\n",
    "def simplify_text_by_deletion(text):\n",
    "    \"\"\"Simplify text by removing non-essential information\"\"\"\n",
    "    non_essential_spans = identify_non_essential_phrases(text)\n",
    "    \n",
    "    if not non_essential_spans:\n",
    "        return text, []  # No non-essential parts found\n",
    "    \n",
    "    # Sort spans in reverse order to delete from end to beginning\n",
    "    # (so earlier deletions don't affect the positions of later ones)\n",
    "    non_essential_spans.sort(reverse=True)\n",
    "    \n",
    "    deletions = []\n",
    "    simplified_text = text\n",
    "    \n",
    "    for start, end, span_type in non_essential_spans:\n",
    "        deleted_text = text[start:end]\n",
    "        simplified_text = simplified_text[:start] + simplified_text[end:]\n",
    "        deletions.append((deleted_text, span_type))\n",
    "        \n",
    "        # Clean up any double spaces or double commas created by the deletion\n",
    "        simplified_text = re.sub(r'\\s+', ' ', simplified_text)\n",
    "        simplified_text = re.sub(r',,', ',', simplified_text)\n",
    "        simplified_text = re.sub(r',\\s*,', ',', simplified_text)\n",
    "        simplified_text = re.sub(r'\\s+\\.', '.', simplified_text)\n",
    "    \n",
    "    return simplified_text, deletions\n",
    "\n",
    "# Test the deletion-based simplification\n",
    "print(\"Testing deletion-based simplification:\")\n",
    "\n",
    "# Correctly access the validation dataset\n",
    "validation_data = asset[\"validation\"]\n",
    "\n",
    "for i in range(5):  # Get the first 5 examples\n",
    "    # Get example by index\n",
    "    example = validation_data[i]\n",
    "    original = example['original']\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {original}\")\n",
    "    \n",
    "    # Simplify by deletion\n",
    "    simplified, deletions = simplify_text_by_deletion(original)\n",
    "    \n",
    "    print(f\"Simplified: {simplified}\")\n",
    "    \n",
    "    if deletions:\n",
    "        print(\"Deleted content:\")\n",
    "        for content, span_type in deletions:\n",
    "            print(f\"  - '{content}' ({span_type})\")\n",
    "    \n",
    "    # Calculate complexity reduction\n",
    "    orig_complexity = sentence_complexity_score(original)\n",
    "    del_complexity = sentence_complexity_score(simplified)\n",
    "    reduction = orig_complexity - del_complexity\n",
    "    \n",
    "    print(f\"Complexity: {orig_complexity:.1f} → {del_complexity:.1f} ({reduction:.1f} reduction)\")\n",
    "    \n",
    "    # Compare to human simplification\n",
    "    human_simplified = example['simplifications'][0]\n",
    "    print(f\"Human simplification: {human_simplified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating simplifications using SARI metric:\n",
      "\n",
      "Example 1:\n",
      "Original: Adjacent counties are Marin (to the south), Mendocino (to the north), Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).\n",
      "System: Adjacent counties are Marin , and Solano and Contra Costa.\n",
      "Reference 1: countries next to it are Marin, Mendocino, Lake, Napa, Solano, and Contra Costa.\n",
      "SARI: 14.82%\n",
      "  Keep: 38.35%\n",
      "  Delete: 0.00%\n",
      "  Add: 6.12%\n",
      "\n",
      "Example 2:\n",
      "Original: A Georgian inscription around the drum attests his name.\n",
      "System: A Georgian inscription around the drum attests his name.\n",
      "Reference 1: A writing around the drum confirms his name.\n",
      "SARI: 26.60%\n",
      "  Keep: 79.79%\n",
      "  Delete: 0.00%\n",
      "  Add: 0.00%\n",
      "\n",
      "Example 3:\n",
      "Original: They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", introducing two new variants of the race; the Cyber-Shades and the Cyber-King.\n",
      "System: They would later return to the revived series in the 2008 Christmas Special \"The Next Doctor\", introducing two new variants of the race; the Cyber-Shades and the Cyber-King.\n",
      "Reference 1: They would return to the new series in the 2008 Special \"The Next Doctor\". There were two new variants; Cyber-Shades and the Cyber-King.\n",
      "SARI: 40.52%\n",
      "  Keep: 96.55%\n",
      "  Delete: 25.00%\n",
      "  Add: 0.00%\n",
      "\n",
      "Average scores over 10 examples:\n",
      "SARI: 37.03%\n",
      "Keep: 82.49%\n",
      "Delete: 27.99%\n",
      "Add: 0.61%\n"
     ]
    }
   ],
   "source": [
    "# First, let's define the combined simplification function that was missing\n",
    "\n",
    "def simplify_text_combined(text, word_complexity_threshold=60, deletion_enabled=True):\n",
    "    \"\"\"\n",
    "    Combined text simplification approach using multiple strategies.\n",
    "    \n",
    "    Args:\n",
    "        text: Original text to simplify\n",
    "        word_complexity_threshold: Complexity threshold for word substitution\n",
    "        deletion_enabled: Whether to use deletion-based simplification\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with simplified text and details about applied simplifications\n",
    "    \"\"\"\n",
    "    # Start with the original text\n",
    "    simplified_text = text\n",
    "    applied_simplifications = {\n",
    "        'word_substitutions': [],\n",
    "        'deletions': []\n",
    "    }\n",
    "    \n",
    "    # 1. First apply word substitution\n",
    "    simplified_text, substitutions = simplify_text_by_word_substitution(simplified_text, \n",
    "                                                                       complexity_threshold=word_complexity_threshold)\n",
    "    applied_simplifications['word_substitutions'] = substitutions\n",
    "    \n",
    "    # 2. Then apply deletion-based simplification if enabled\n",
    "    if deletion_enabled:\n",
    "        simplified_text, deletions = simplify_text_by_deletion(simplified_text)\n",
    "        applied_simplifications['deletions'] = deletions\n",
    "    \n",
    "    # Measure complexity reduction\n",
    "    original_complexity = sentence_complexity_score(text)\n",
    "    final_complexity = sentence_complexity_score(simplified_text)\n",
    "    complexity_reduction = original_complexity - final_complexity\n",
    "    \n",
    "    # Calculate meaning preservation\n",
    "    try:\n",
    "        # Use semantic similarity function (previously defined)\n",
    "        meaning_preservation = calculate_semantic_similarity(text, simplified_text)\n",
    "    except:\n",
    "        # Fallback if there's an issue with the semantic similarity function\n",
    "        meaning_preservation = None\n",
    "    \n",
    "    # Return the result with metadata\n",
    "    return {\n",
    "        'original_text': text,\n",
    "        'simplified_text': simplified_text,\n",
    "        'original_complexity': original_complexity,\n",
    "        'simplified_complexity': final_complexity,\n",
    "        'complexity_reduction': complexity_reduction,\n",
    "        'meaning_preservation': meaning_preservation,\n",
    "        'applied_simplifications': applied_simplifications\n",
    "    }\n",
    "\n",
    "# 10. Evaluation Using SARI Metric\n",
    "def compute_ngrams(text, n):\n",
    "    \"\"\"Compute n-grams for a text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return set(ngrams)\n",
    "\n",
    "def compute_precision(system_ngrams, reference_ngrams):\n",
    "    \"\"\"Compute precision: what fraction of system n-grams are in the references.\"\"\"\n",
    "    if not system_ngrams:\n",
    "        return 1.0  # If system has no n-grams, precision is 1.0\n",
    "    return len(system_ngrams.intersection(reference_ngrams)) / len(system_ngrams)\n",
    "\n",
    "def compute_recall(system_ngrams, reference_ngrams):\n",
    "    \"\"\"Compute recall: what fraction of reference n-grams are in the system.\"\"\"\n",
    "    if not reference_ngrams:\n",
    "        return 1.0  # If reference has no n-grams, recall is 1.0\n",
    "    return len(system_ngrams.intersection(reference_ngrams)) / len(reference_ngrams)\n",
    "\n",
    "def compute_f1(precision, recall):\n",
    "    \"\"\"Compute F1 score: harmonic mean of precision and recall.\"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def compute_sari(original, system, references, n=4):\n",
    "    \"\"\"\n",
    "    Compute SARI score for system simplification compared to original and references.\n",
    "    \n",
    "    Args:\n",
    "        original: Original sentence\n",
    "        system: System simplification\n",
    "        references: List of reference simplifications\n",
    "        n: Maximum n-gram size to consider\n",
    "    \n",
    "    Returns:\n",
    "        SARI score (average of add, keep, and delete F1 scores)\n",
    "    \"\"\"\n",
    "    add_scores = []\n",
    "    keep_scores = []\n",
    "    del_scores = []\n",
    "    \n",
    "    # Process all n-grams from unigrams to n-grams\n",
    "    for k in range(1, n + 1):\n",
    "        original_ngrams = compute_ngrams(original, k)\n",
    "        system_ngrams = compute_ngrams(system, k)\n",
    "        \n",
    "        # Compute n-grams for all references\n",
    "        reference_ngrams_list = [compute_ngrams(ref, k) for ref in references]\n",
    "        \n",
    "        # Compute union of reference n-grams\n",
    "        reference_ngrams_union = set()\n",
    "        for ref_ngrams in reference_ngrams_list:\n",
    "            reference_ngrams_union.update(ref_ngrams)\n",
    "        \n",
    "        # KEEP: n-grams in both original and system output\n",
    "        keep_precision = compute_precision(system_ngrams.intersection(original_ngrams), \n",
    "                                         reference_ngrams_union.intersection(original_ngrams))\n",
    "        keep_recall = compute_recall(system_ngrams.intersection(original_ngrams), \n",
    "                                   reference_ngrams_union.intersection(original_ngrams))\n",
    "        keep_f1 = compute_f1(keep_precision, keep_recall)\n",
    "        keep_scores.append(keep_f1)\n",
    "        \n",
    "        # DELETE: n-grams in original but not in system output\n",
    "        del_precision = compute_precision(original_ngrams - system_ngrams, \n",
    "                                        original_ngrams - reference_ngrams_union)\n",
    "        del_recall = compute_recall(original_ngrams - system_ngrams, \n",
    "                                  original_ngrams - reference_ngrams_union)\n",
    "        del_f1 = compute_f1(del_precision, del_recall)\n",
    "        del_scores.append(del_f1)\n",
    "        \n",
    "        # ADD: n-grams in system output but not in original\n",
    "        add_precision = compute_precision(system_ngrams - original_ngrams, \n",
    "                                        reference_ngrams_union - original_ngrams)\n",
    "        add_recall = compute_recall(system_ngrams - original_ngrams, \n",
    "                                  reference_ngrams_union - original_ngrams)\n",
    "        add_f1 = compute_f1(add_precision, add_recall)\n",
    "        add_scores.append(add_f1)\n",
    "    \n",
    "    # Average over n-gram sizes\n",
    "    keep_score = sum(keep_scores) / len(keep_scores)\n",
    "    del_score = sum(del_scores) / len(del_scores)\n",
    "    add_score = sum(add_scores) / len(add_scores)\n",
    "    \n",
    "    # Final SARI score is average of the three operations\n",
    "    sari_score = (add_score + keep_score + del_score) / 3\n",
    "    \n",
    "    return {\n",
    "        'sari': sari_score * 100,  # Convert to percentage\n",
    "        'keep': keep_score * 100,\n",
    "        'delete': del_score * 100,\n",
    "        'add': add_score * 100\n",
    "    }\n",
    "\n",
    "# Test SARI evaluation on a subset of examples\n",
    "print(\"Evaluating simplifications using SARI metric:\")\n",
    "\n",
    "# Correctly access the validation dataset\n",
    "validation_data = asset[\"validation\"]\n",
    "\n",
    "# Collect SARI scores for our simplification approach\n",
    "sari_scores = []\n",
    "\n",
    "for i in range(10):  # Using 10 examples for a more reliable average\n",
    "    # Get example by index\n",
    "    example = validation_data[i]\n",
    "    original = example['original']\n",
    "    references = example['simplifications']\n",
    "    \n",
    "    # Simplify using our combined approach\n",
    "    result = simplify_text_combined(original)\n",
    "    system_output = result['simplified_text']\n",
    "    \n",
    "    # Compute SARI\n",
    "    sari_result = compute_sari(original, system_output, references)\n",
    "    \n",
    "    # Store the result\n",
    "    sari_scores.append(sari_result)\n",
    "    \n",
    "    # Print detailed results for the first few examples\n",
    "    if i < 3:\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Original: {original}\")\n",
    "        print(f\"System: {system_output}\")\n",
    "        print(f\"Reference 1: {references[0]}\")\n",
    "        print(f\"SARI: {sari_result['sari']:.2f}%\")\n",
    "        print(f\"  Keep: {sari_result['keep']:.2f}%\")\n",
    "        print(f\"  Delete: {sari_result['delete']:.2f}%\")\n",
    "        print(f\"  Add: {sari_result['add']:.2f}%\")\n",
    "\n",
    "# Calculate average SARI scores\n",
    "avg_sari = sum(s['sari'] for s in sari_scores) / len(sari_scores)\n",
    "avg_keep = sum(s['keep'] for s in sari_scores) / len(sari_scores)\n",
    "avg_delete = sum(s['delete'] for s in sari_scores) / len(sari_scores)\n",
    "avg_add = sum(s['add'] for s in sari_scores) / len(sari_scores)\n",
    "\n",
    "print(f\"\\nAverage scores over {len(sari_scores)} examples:\")\n",
    "print(f\"SARI: {avg_sari:.2f}%\")\n",
    "print(f\"Keep: {avg_keep:.2f}%\")\n",
    "print(f\"Delete: {avg_delete:.2f}%\")\n",
    "print(f\"Add: {avg_add:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing definition-based simplification:\n",
      "\n",
      "Example 1:\n",
      "Original: The ubiquitous nature of smartphones has revolutionized modern communication.\n",
      "Simplified: The ubiquitous nature of smartphones has revolutionized modern communication.\n",
      "Complexity: 63.6 → 63.6\n",
      "\n",
      "Example 2:\n",
      "Original: The protagonist's epiphany led to a profound transformation in his worldview.\n",
      "Simplified: The protagonist 's epiphany led to a profound transformation in his worldview.\n",
      "Complexity: 60.4 → 60.9\n",
      "\n",
      "Example 3:\n",
      "Original: The government implemented austerity measures to address the fiscal deficit.\n",
      "Simplified: The government implemented austerity measures to address the fiscal deficit.\n",
      "Complexity: 59.6 → 59.6\n",
      "\n",
      "Testing enhanced combined simplification with definitions:\n",
      "\n",
      "Example 1:\n",
      "Original: The ubiquitous nature of smartphones has revolutionized modern communication.\n",
      "Simplified: The ubiquitous nature of smartphones has inspire modern communication.\n",
      "Complexity: 63.6 → 57.2\n",
      "\n",
      "Word Substitutions:\n",
      "  - 'revolutionized' → 'inspire' (complexity: 43.0 → 20.0)\n",
      "\n",
      "Example 2:\n",
      "Original: The protagonist's epiphany led to a profound transformation in his worldview.\n",
      "Simplified: The friend 's epiphany led to a sound shift in his worldview.\n",
      "Complexity: 60.4 → 45.5\n",
      "\n",
      "Word Substitutions:\n",
      "  - 'transformation' → 'shift' (complexity: 40.0 → 13.0)\n",
      "  - 'protagonist' → 'friend' (complexity: 34.0 → 15.0)\n",
      "  - 'profound' → 'sound' (complexity: 22.0 → 13.0)\n",
      "\n",
      "Example 3:\n",
      "Original: The government implemented austerity measures to address the fiscal deficit.\n",
      "Simplified: The regime apply austerity bar to address the fiscal shortage.\n",
      "Complexity: 59.6 → 47.8\n",
      "\n",
      "Word Substitutions:\n",
      "  - 'implemented' → 'apply' (complexity: 34.0 → 16.0)\n",
      "  - 'government' → 'regime' (complexity: 29.0 → 18.0)\n",
      "  - 'deficit' → 'shortage' (complexity: 23.0 → 22.0)\n",
      "  - 'measures' → 'bar' (complexity: 22.0 → 9.0)\n",
      "\n",
      "Definitions Added:\n",
      "  - Added definition for 'austerity' (complexity: 30.0)\n",
      "\n",
      "Deletions:\n",
      "  - Deleted '(meaning the trait of great self-denial)' (parenthetical)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kennethshyle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Definition-Based Simplification\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Ensure WordNet is downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "def get_simple_definition(word, pos=None):\n",
    "    \"\"\"\n",
    "    Get a simple definition for a word from WordNet.\n",
    "    \n",
    "    Args:\n",
    "        word: The word to define\n",
    "        pos: Part of speech (optional, can be 'n', 'v', 'a', 'r' for noun, verb, adj, adverb)\n",
    "        \n",
    "    Returns:\n",
    "        A simplified definition or None if not found\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Map NLTK POS tags to WordNet POS tags\n",
    "    pos_map = {\n",
    "        'NN': wordnet.NOUN,\n",
    "        'NNS': wordnet.NOUN,\n",
    "        'NNP': wordnet.NOUN,\n",
    "        'NNPS': wordnet.NOUN,\n",
    "        'VB': wordnet.VERB,\n",
    "        'VBD': wordnet.VERB,\n",
    "        'VBG': wordnet.VERB,\n",
    "        'VBN': wordnet.VERB,\n",
    "        'VBP': wordnet.VERB,\n",
    "        'VBZ': wordnet.VERB,\n",
    "        'JJ': wordnet.ADJ,\n",
    "        'JJR': wordnet.ADJ,\n",
    "        'JJS': wordnet.ADJ,\n",
    "        'RB': wordnet.ADV,\n",
    "        'RBR': wordnet.ADV,\n",
    "        'RBS': wordnet.ADV\n",
    "    }\n",
    "    \n",
    "    if pos in pos_map:\n",
    "        pos = pos_map[pos]\n",
    "    \n",
    "    # Get synsets from WordNet\n",
    "    synsets = wordnet.synsets(word, pos=pos) if pos else wordnet.synsets(word)\n",
    "    \n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    # Get the most common meaning (first synset)\n",
    "    # For more sophisticated applications, word sense disambiguation would be needed\n",
    "    definition = synsets[0].definition()\n",
    "    \n",
    "    # Simplify the definition\n",
    "    simple_def = simplify_definition(definition)\n",
    "    \n",
    "    # Only use definitions that are actually simpler than the original word\n",
    "    if simple_def and word_complexity_score(word) > sentence_complexity_score(simple_def):\n",
    "        return simple_def\n",
    "    \n",
    "    return None\n",
    "\n",
    "def simplify_definition(definition):\n",
    "    \"\"\"Simplify a dictionary definition to make it more readable\"\"\"\n",
    "    # Remove nested clauses and parenthetical information\n",
    "    simplified = re.sub(r'\\([^)]*\\)', '', definition)\n",
    "    \n",
    "    # Split into words for analysis\n",
    "    words = word_tokenize(simplified)\n",
    "    \n",
    "    # Check if the definition itself has complex words\n",
    "    complex_words = []\n",
    "    for word in words:\n",
    "        if len(word) > 3 and word.isalpha() and word_complexity_score(word) > 70:\n",
    "            complex_words.append(word)\n",
    "    \n",
    "    # If the definition has too many complex words, it might not be helpful\n",
    "    if len(complex_words) > 2:\n",
    "        return None\n",
    "    \n",
    "    # Remove unnecessary phrases like \"of a\" or \"pertaining to\"\n",
    "    simplified = re.sub(r'\\bpertaining to\\b', '', simplified)\n",
    "    simplified = re.sub(r'\\bconsisting of\\b', 'is', simplified)\n",
    "    simplified = re.sub(r'\\brelating to\\b', 'about', simplified)\n",
    "    \n",
    "    # Clean up and return\n",
    "    simplified = re.sub(r'\\s+', ' ', simplified).strip()\n",
    "    \n",
    "    # Only return if it's a reasonably short definition\n",
    "    if len(word_tokenize(simplified)) <= 10:\n",
    "        return simplified\n",
    "    \n",
    "    return None\n",
    "\n",
    "def simplify_text_by_splitting(text):\n",
    "    \"\"\"\n",
    "    Simplify text by splitting long sentences into shorter ones.\n",
    "    \n",
    "    This function looks for coordinating conjunctions and common discourse \n",
    "    markers to split sentences.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "        \n",
    "    # Split text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    simplified_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # If the sentence is short enough, keep it as is\n",
    "        if len(word_tokenize(sentence)) <= 15:\n",
    "            simplified_sentences.append(sentence)\n",
    "            continue\n",
    "            \n",
    "        # Look for places to split the sentence\n",
    "        split_markers = [\n",
    "            \" and \", \" but \", \" or \", \" nor \", \" so \", \" for \", \" yet \",\n",
    "            \" however, \", \" moreover, \", \" furthermore, \", \" consequently, \",\n",
    "            \" nevertheless, \", \" in addition, \", \" in fact, \"\n",
    "        ]\n",
    "        \n",
    "        # Try to split at these markers\n",
    "        splits = []\n",
    "        for marker in split_markers:\n",
    "            if marker in sentence:\n",
    "                parts = sentence.split(marker, 1)\n",
    "                # Only split if both parts are meaningful (at least 3 words)\n",
    "                if (len(word_tokenize(parts[0])) >= 3 and \n",
    "                    len(word_tokenize(parts[1])) >= 3):\n",
    "                    # Make the first part a complete sentence\n",
    "                    splits = [parts[0] + \".\", marker.strip() + \" \" + parts[1]]\n",
    "                    break\n",
    "                    \n",
    "        if splits:\n",
    "            # If we found a good place to split, use the splits\n",
    "            simplified_sentences.extend(splits)\n",
    "        else:\n",
    "            # Otherwise keep the original sentence\n",
    "            simplified_sentences.append(sentence)\n",
    "    \n",
    "    # Join the sentences back together\n",
    "    simplified_text = \" \".join(simplified_sentences)\n",
    "    \n",
    "    # Clean up any double periods\n",
    "    simplified_text = simplified_text.replace(\"..\", \".\")\n",
    "    \n",
    "    return simplified_text\n",
    "\n",
    "def simplify_text_with_definitions(text, complexity_threshold=75):\n",
    "    \"\"\"\n",
    "    Simplify text by replacing complex words with their definitions\n",
    "    \n",
    "    Args:\n",
    "        text: Text to simplify\n",
    "        complexity_threshold: Only replace words above this complexity\n",
    "        \n",
    "    Returns:\n",
    "        Simplified text and list of replacements\n",
    "    \"\"\"\n",
    "    # Tokenize and get part of speech\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    replacements = []\n",
    "    result_tokens = tokens.copy()\n",
    "    \n",
    "    for i, (word, pos) in enumerate(pos_tags):\n",
    "        # Only process content words (nouns, verbs, adjectives, adverbs)\n",
    "        if not pos.startswith(('NN', 'VB', 'JJ', 'RB')) or len(word) <= 4:\n",
    "            continue\n",
    "            \n",
    "        # Check if word is complex enough to replace\n",
    "        complexity = word_complexity_score(word)\n",
    "        if complexity < complexity_threshold:\n",
    "            continue\n",
    "            \n",
    "        # Try to get a simple definition\n",
    "        definition = get_simple_definition(word, pos)\n",
    "        if definition:\n",
    "            # Format the replacement to fit into the sentence\n",
    "            formatted_def = f\"{word} (meaning {definition})\"\n",
    "            \n",
    "            # Store the replacement\n",
    "            replacements.append((word, formatted_def, complexity))\n",
    "            \n",
    "            # Update the token\n",
    "            result_tokens[i] = formatted_def\n",
    "    \n",
    "    # Reconstruct the text with replacements\n",
    "    simplified_text = ' '.join(result_tokens)\n",
    "    \n",
    "    # Fix spacing around punctuation\n",
    "    simplified_text = re.sub(r'\\s+([.,;:!?])', r'\\1', simplified_text)\n",
    "    \n",
    "    return simplified_text, replacements\n",
    "\n",
    "# Test the definition-based simplification\n",
    "print(\"Testing definition-based simplification:\")\n",
    "\n",
    "# Test on a few examples\n",
    "test_sentences = [\n",
    "    \"The ubiquitous nature of smartphones has revolutionized modern communication.\",\n",
    "    \"The protagonist's epiphany led to a profound transformation in his worldview.\",\n",
    "    \"The government implemented austerity measures to address the fiscal deficit.\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {sentence}\")\n",
    "    \n",
    "    # Simplify using definitions\n",
    "    simplified, replacements = simplify_text_with_definitions(sentence)\n",
    "    \n",
    "    print(f\"Simplified: {simplified}\")\n",
    "    \n",
    "    if replacements:\n",
    "        print(\"Replacements:\")\n",
    "        for word, replacement, complexity in replacements:\n",
    "            print(f\"  - '{word}' (complexity: {complexity:.1f}) → '{replacement}'\")\n",
    "    \n",
    "    # Calculate complexity reduction\n",
    "    orig_complexity = sentence_complexity_score(sentence)\n",
    "    simp_complexity = sentence_complexity_score(simplified)\n",
    "    print(f\"Complexity: {orig_complexity:.1f} → {simp_complexity:.1f}\")\n",
    "\n",
    "# Update the combined simplification function to include definition-based simplification\n",
    "def simplify_text_combined_with_definitions(text, word_complexity_threshold=20, \n",
    "                                           definition_complexity_threshold=25,\n",
    "                                           deletion_enabled=True, \n",
    "                                           definitions_enabled=True):\n",
    "    \"\"\"\n",
    "    Enhanced combined text simplification approach that includes definition-based simplification.\n",
    "    \n",
    "    Args:\n",
    "        text: Original text to simplify\n",
    "        word_complexity_threshold: Complexity threshold for word substitution\n",
    "        definition_complexity_threshold: Complexity threshold for definition replacement\n",
    "        deletion_enabled: Whether to use deletion-based simplification\n",
    "        definitions_enabled: Whether to use definition-based simplification\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with simplified text and details about applied simplifications\n",
    "    \"\"\"\n",
    "    # Start with the original text\n",
    "    simplified_text = text\n",
    "    applied_simplifications = {\n",
    "        'word_substitutions': [],\n",
    "        'definitions_added': [],\n",
    "        'deletions': []\n",
    "    }\n",
    "    \n",
    "    # 1. First apply word substitution\n",
    "    simplified_text, substitutions = simplify_text_by_word_substitution(\n",
    "        simplified_text, complexity_threshold=word_complexity_threshold\n",
    "    )\n",
    "    applied_simplifications['word_substitutions'] = substitutions\n",
    "    \n",
    "    # 2. Then apply definition-based simplification if enabled\n",
    "    if definitions_enabled:\n",
    "        simplified_text, definitions = simplify_text_with_definitions(\n",
    "            simplified_text, complexity_threshold=definition_complexity_threshold\n",
    "        )\n",
    "        applied_simplifications['definitions_added'] = definitions\n",
    "    \n",
    "    # 3. Finally apply deletion-based simplification if enabled\n",
    "    if deletion_enabled:\n",
    "        simplified_text, deletions = simplify_text_by_deletion(simplified_text)\n",
    "        applied_simplifications['deletions'] = deletions\n",
    "    \n",
    "    # Measure complexity reduction\n",
    "    original_complexity = sentence_complexity_score(text)\n",
    "    final_complexity = sentence_complexity_score(simplified_text)\n",
    "    complexity_reduction = original_complexity - final_complexity\n",
    "    \n",
    "    # Calculate meaning preservation\n",
    "    try:\n",
    "        # Use semantic similarity function (previously defined)\n",
    "        meaning_preservation = calculate_semantic_similarity(text, simplified_text)\n",
    "    except:\n",
    "        # Fallback if there's an issue with the semantic similarity function\n",
    "        meaning_preservation = None\n",
    "    \n",
    "    # Return the result with metadata\n",
    "    return {\n",
    "        'original_text': text,\n",
    "        'simplified_text': simplified_text,\n",
    "        'original_complexity': original_complexity,\n",
    "        'simplified_complexity': final_complexity,\n",
    "        'complexity_reduction': complexity_reduction,\n",
    "        'meaning_preservation': meaning_preservation,\n",
    "        'applied_simplifications': applied_simplifications\n",
    "    }\n",
    "\n",
    "# Test the enhanced combined simplification\n",
    "print(\"\\nTesting enhanced combined simplification with definitions:\")\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {sentence}\")\n",
    "    \n",
    "    # Simplify using the enhanced combined approach\n",
    "    result = simplify_text_combined_with_definitions(sentence)\n",
    "    \n",
    "    print(f\"Simplified: {result['simplified_text']}\")\n",
    "    print(f\"Complexity: {result['original_complexity']:.1f} → {result['simplified_complexity']:.1f}\")\n",
    "    \n",
    "    # Print the applied simplifications\n",
    "    for category, simplifications in result['applied_simplifications'].items():\n",
    "        if simplifications:\n",
    "            print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "            if category == 'word_substitutions':\n",
    "                for word, simpler, orig_score, simp_score in simplifications:\n",
    "                    print(f\"  - '{word}' → '{simpler}' (complexity: {orig_score:.1f} → {simp_score:.1f})\")\n",
    "            elif category == 'definitions_added':\n",
    "                for word, definition, complexity in simplifications:\n",
    "                    print(f\"  - Added definition for '{word}' (complexity: {complexity:.1f})\")\n",
    "            elif category == 'deletions':\n",
    "                for content, span_type in simplifications:\n",
    "                    print(f\"  - Deleted '{content}' ({span_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved tree search simplification:\n",
      "\n",
      "Example 1:\n",
      "Original: The ubiquitous nature of smartphones has revolutionized modern communication.\n",
      "Word complexities:\n",
      "  - 'revolutionized': 43.0\n",
      "  - 'communication': 41.0\n",
      "  - 'ubiquitous': 32.0\n",
      "  - 'smartphones': 31.0\n",
      "\n",
      "  Config: Low complexity target\n",
      "  Simplified: The ubiquitous nature of smartphones has inspire modern communication.\n",
      "  Complexity: 63.6 → 57.2\n",
      "  Meaning preservation: 0.77\n",
      "  Operations: 1\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 1 words\n",
      "      New complexity: 57.2\n",
      "      Meaning preservation: 0.77\n",
      "      Word substitutions:\n",
      "        'revolutionized' → 'inspire' (43.0 → 20.0)\n",
      "\n",
      "  Config: Medium complexity target\n",
      "  Simplified: The ubiquitous nature of smartphones has inspire modern communication.\n",
      "  Complexity: 63.6 → 57.2\n",
      "  Meaning preservation: 0.77\n",
      "  Operations: 1\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 1 words\n",
      "      New complexity: 57.2\n",
      "      Meaning preservation: 0.77\n",
      "      Word substitutions:\n",
      "        'revolutionized' → 'inspire' (43.0 → 20.0)\n",
      "\n",
      "  Config: High complexity target\n",
      "  Simplified: The ubiquitous nature of smartphones has inspire modern communication.\n",
      "  Complexity: 63.6 → 57.2\n",
      "  Meaning preservation: 0.77\n",
      "  Operations: 1\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 1 words\n",
      "      New complexity: 57.2\n",
      "      Meaning preservation: 0.77\n",
      "      Word substitutions:\n",
      "        'revolutionized' → 'inspire' (43.0 → 20.0)\n",
      "\n",
      "Example 2:\n",
      "Original: The protagonist's epiphany led to a profound transformation in his worldview.\n",
      "Word complexities:\n",
      "  - 'transformation': 40.0\n",
      "  - 'protagonist': 34.0\n",
      "  - 'epiphany': 28.0\n",
      "  - 'worldview': 24.0\n",
      "  - 'profound': 22.0\n",
      "\n",
      "  Config: Low complexity target\n",
      "  Simplified: The friend's epiphany led to a sound shift in his worldview.\n",
      "  Complexity: 60.4 → 45.0\n",
      "  Meaning preservation: 0.67\n",
      "  Operations: 1\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 3 words\n",
      "      New complexity: 45.0\n",
      "      Meaning preservation: 0.67\n",
      "      Word substitutions:\n",
      "        'transformation' → 'shift' (40.0 → 13.0)\n",
      "        'protagonist' → 'friend' (34.0 → 15.0)\n",
      "        'profound' → 'sound' (22.0 → 13.0)\n",
      "\n",
      "  Config: Medium complexity target\n",
      "  Simplified: The friend's epiphany led to a sound shift in his worldview.\n",
      "  Complexity: 60.4 → 45.0\n",
      "  Meaning preservation: 0.67\n",
      "  Operations: 1\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 3 words\n",
      "      New complexity: 45.0\n",
      "      Meaning preservation: 0.67\n",
      "      Word substitutions:\n",
      "        'transformation' → 'shift' (40.0 → 13.0)\n",
      "        'protagonist' → 'friend' (34.0 → 15.0)\n",
      "        'profound' → 'sound' (22.0 → 13.0)\n",
      "\n",
      "  Config: High complexity target\n",
      "  Simplified: The protagonist's epiphany led to a profound transformation in his worldview.\n",
      "  Complexity: 60.4 → 60.4\n",
      "  Meaning preservation: 1.00\n",
      "  Operations: 0\n",
      "\n",
      "Example 3:\n",
      "Original: The government implemented austerity measures to address the fiscal deficit.\n",
      "Word complexities:\n",
      "  - 'implemented': 34.0\n",
      "  - 'austerity': 30.0\n",
      "  - 'government': 29.0\n",
      "  - 'deficit': 23.0\n",
      "  - 'measures': 22.0\n",
      "\n",
      "  Config: Low complexity target\n",
      "  Simplified: The regime apply austerity bar to address the fiscal dearth.\n",
      "  Complexity: 59.6 → 46.0\n",
      "  Meaning preservation: 0.58\n",
      "  Operations: 2\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 4 words\n",
      "      New complexity: 47.8\n",
      "      Meaning preservation: 0.58\n",
      "      Word substitutions:\n",
      "        'implemented' → 'apply' (34.0 → 16.0)\n",
      "        'government' → 'regime' (29.0 → 18.0)\n",
      "        'deficit' → 'shortage' (23.0 → 22.0)\n",
      "        'measures' → 'bar' (22.0 → 9.0)\n",
      "    Step 2: Applied substitution - substituted 1 words\n",
      "      New complexity: 46.0\n",
      "      Meaning preservation: 0.58\n",
      "      Word substitutions:\n",
      "        'shortage' → 'dearth' (22.0 → 15.0)\n",
      "\n",
      "  Config: Medium complexity target\n",
      "  Simplified: The government implemented austerity measures to address the fiscal deficit.\n",
      "  Complexity: 59.6 → 59.6\n",
      "  Meaning preservation: 1.00\n",
      "  Operations: 0\n",
      "\n",
      "  Config: High complexity target\n",
      "  Simplified: The government implemented austerity measures to address the fiscal deficit.\n",
      "  Complexity: 59.6 → 59.6\n",
      "  Meaning preservation: 1.00\n",
      "  Operations: 0\n",
      "\n",
      "Example 4:\n",
      "Original: This is the quintessential understanding for text simplification\n",
      "Word complexities:\n",
      "  - 'simplification': 43.0\n",
      "  - 'quintessential': 40.0\n",
      "  - 'understanding': 38.0\n",
      "\n",
      "  Config: Low complexity target\n",
      "  Simplified: This is the quintessential see for text fall\n",
      "  Complexity: 55.0 → 36.3\n",
      "  Meaning preservation: 0.54\n",
      "  Operations: 3\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 2 words\n",
      "      New complexity: 41.1\n",
      "      Meaning preservation: 0.54\n",
      "      Word substitutions:\n",
      "        'simplification' → 'reduction' (43.0 → 27.0)\n",
      "        'understanding' → 'see' (38.0 → 9.0)\n",
      "    Step 2: Applied substitution - substituted 1 words\n",
      "      New complexity: 39.9\n",
      "      Meaning preservation: 0.54\n",
      "      Word substitutions:\n",
      "        'reduction' → 'decrease' (27.0 → 22.0)\n",
      "    Step 3: Applied substitution - substituted 1 words\n",
      "      New complexity: 36.3\n",
      "      Meaning preservation: 0.54\n",
      "      Word substitutions:\n",
      "        'decrease' → 'fall' (22.0 → 11.0)\n",
      "\n",
      "  Config: Medium complexity target\n",
      "  Simplified: This is the quintessential understanding for text simplification\n",
      "  Complexity: 55.0 → 55.0\n",
      "  Meaning preservation: 1.00\n",
      "  Operations: 0\n",
      "\n",
      "  Config: High complexity target\n",
      "  Simplified: This is the quintessential understanding for text simplification\n",
      "  Complexity: 55.0 → 55.0\n",
      "  Meaning preservation: 1.00\n",
      "  Operations: 0\n",
      "\n",
      "Example 5:\n",
      "Original: This was an absolutely smashing success and I wish for nothing but the best for you two\n",
      "Word complexities:\n",
      "  - 'absolutely': 32.0\n",
      "  - 'smashing': 22.0\n",
      "\n",
      "  Config: Low complexity target\n",
      "  Simplified: This was an dead boom success and I wish for nothing but the best for you two\n",
      "  Complexity: 56.8 → 49.6\n",
      "  Meaning preservation: 0.86\n",
      "  Operations: 1\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 2 words\n",
      "      New complexity: 49.6\n",
      "      Meaning preservation: 0.86\n",
      "      Word substitutions:\n",
      "        'absolutely' → 'dead' (32.0 → 11.0)\n",
      "        'smashing' → 'boom' (22.0 → 11.0)\n",
      "\n",
      "  Config: Medium complexity target\n",
      "  Simplified: This was an dead boom success and I wish for nothing but the best for you two\n",
      "  Complexity: 56.8 → 49.6\n",
      "  Meaning preservation: 0.86\n",
      "  Operations: 1\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 2 words\n",
      "      New complexity: 49.6\n",
      "      Meaning preservation: 0.86\n",
      "      Word substitutions:\n",
      "        'absolutely' → 'dead' (32.0 → 11.0)\n",
      "        'smashing' → 'boom' (22.0 → 11.0)\n",
      "\n",
      "  Config: High complexity target\n",
      "  Simplified: This was an dead boom success and I wish for nothing but the best for you two\n",
      "  Complexity: 56.8 → 49.6\n",
      "  Meaning preservation: 0.86\n",
      "  Operations: 1\n",
      "\n",
      "  Simplification path:\n",
      "    Step 0: Original\n",
      "    Step 1: Applied substitution - substituted 2 words\n",
      "      New complexity: 49.6\n",
      "      Meaning preservation: 0.86\n",
      "      Word substitutions:\n",
      "        'absolutely' → 'dead' (32.0 → 11.0)\n",
      "        'smashing' → 'boom' (22.0 → 11.0)\n"
     ]
    }
   ],
   "source": [
    "# Improved Tree Search Simplification\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Set, Any, Optional\n",
    "\n",
    "@dataclass\n",
    "class SimplificationState:\n",
    "    \"\"\"Class to represent a state in the simplification process\"\"\"\n",
    "    text: str\n",
    "    complexity: float \n",
    "    meaning_preservation: float  \n",
    "    operations_applied: List[Tuple[str, str]]\n",
    "    parent: Optional['SimplificationState'] = None\n",
    "    operation_details: Any = None \n",
    "    \n",
    "    # For priority queue ordering (lower values are higher priority)\n",
    "    def __lt__(self, other):\n",
    "        # Prioritize states with lower complexity but reasonable meaning preservation\n",
    "        return (self.complexity * 0.8 - self.meaning_preservation * 0.2) < (other.complexity * 0.8 - other.meaning_preservation * 0.2)\n",
    "\n",
    "def improved_tree_search_simplification(\n",
    "    text: str, \n",
    "    target_complexity: Optional[float] = None,\n",
    "    max_operations: int = 5,\n",
    "    meaning_threshold: float = 0.6,\n",
    "    beam_width: int = 5, \n",
    "    word_complexity_threshold: float = 50.0, \n",
    "    definition_complexity_threshold: float = 60.0,\n",
    "    verbose: bool = True \n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Enhanced tree search for text simplification with better debugging and more aggressive simplification.\n",
    "    \"\"\"\n",
    "    # Start timing for performance measurement\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Analyze the original text in detail\n",
    "    original_complexity = sentence_complexity_score(text)\n",
    "    complex_words = identify_complex_words(text, threshold=word_complexity_threshold)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Original text complexity: {original_complexity:.1f}\")\n",
    "        print(f\"Found {len(complex_words)} complex words above threshold {word_complexity_threshold}\")\n",
    "        for word, score in complex_words:\n",
    "            print(f\"  - '{word}': {score:.1f}\")\n",
    "    \n",
    "    # Create initial state\n",
    "    initial_state = SimplificationState(\n",
    "        text=text,\n",
    "        complexity=original_complexity,\n",
    "        meaning_preservation=1.0,  # Original text has perfect meaning preservation\n",
    "        operations_applied=[],\n",
    "        parent=None\n",
    "    )\n",
    "    \n",
    "    # Priority queue for beam search\n",
    "    beam = [initial_state]\n",
    "    \n",
    "    # Keep track of the best state found so far\n",
    "    best_state = initial_state\n",
    "    \n",
    "    # If target complexity is specified, track if we've reached it\n",
    "    target_reached = target_complexity is None or original_complexity <= target_complexity\n",
    "    \n",
    "    # Track visited states with operation type to allow different operations on same text\n",
    "    visited_states = set([(text, \"initial\")])\n",
    "    \n",
    "    # Simplification operations to try\n",
    "    def get_possible_operations(state):\n",
    "        operations = []\n",
    "        \n",
    "        # Try word substitution - try lower thresholds if needed\n",
    "        for threshold in [word_complexity_threshold, 40]:  # High (50) and medium (40) complexity\n",
    "            substituted_text, substitutions = simplify_text_by_word_substitution(\n",
    "                state.text, \n",
    "                complexity_threshold=threshold,\n",
    "                verbose=False  # Don't show verbose output here\n",
    "            )\n",
    "            if substituted_text != state.text and substitutions:\n",
    "                operation_key = (\"substitution\", substituted_text)\n",
    "                if operation_key not in visited_states:\n",
    "                    operations.append((\"substitution\", substituted_text, substitutions))\n",
    "                    visited_states.add(operation_key)\n",
    "                    if verbose:\n",
    "                        print(f\"Found word substitution operation with {len(substitutions)} substitutions\")\n",
    "                    break  # Only use one threshold\n",
    "        \n",
    "        # Try definition addition\n",
    "        defined_text, definitions = simplify_text_with_definitions(\n",
    "            state.text, complexity_threshold=definition_complexity_threshold\n",
    "        )\n",
    "        if defined_text != state.text and definitions:\n",
    "            operation_key = (\"definition\", defined_text)\n",
    "            if operation_key not in visited_states:\n",
    "                operations.append((\"definition\", defined_text, definitions))\n",
    "                visited_states.add(operation_key)\n",
    "                if verbose:\n",
    "                    print(f\"Found definition operation with {len(definitions)} definitions\")\n",
    "        \n",
    "        # Try sentence splitting\n",
    "        split_text = simplify_text_by_splitting(state.text)\n",
    "        if split_text != state.text:\n",
    "            operation_key = (\"splitting\", split_text)\n",
    "            if operation_key not in visited_states:\n",
    "                operations.append((\"splitting\", split_text, None))\n",
    "                visited_states.add(operation_key)\n",
    "                if verbose:\n",
    "                    print(f\"Found sentence splitting operation\")\n",
    "        \n",
    "        # Try content deletion\n",
    "        deleted_text, deletions = simplify_text_by_deletion(state.text)\n",
    "        if deleted_text != state.text and deletions:\n",
    "            operation_key = (\"deletion\", deleted_text)\n",
    "            if operation_key not in visited_states:\n",
    "                operations.append((\"deletion\", deleted_text, deletions))\n",
    "                visited_states.add(operation_key)\n",
    "                if verbose:\n",
    "                    print(f\"Found deletion operation with {len(deletions)} deletions\")\n",
    "        \n",
    "        return operations\n",
    "    \n",
    "    # Main search loop\n",
    "    for iteration in range(max_operations):\n",
    "        if not beam or target_reached:\n",
    "            if verbose:\n",
    "                print(f\"Stopping search: {'target reached' if target_reached else 'empty beam'}\")\n",
    "            break\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nIteration {iteration+1}/{max_operations}\")\n",
    "            print(f\"Current beam size: {len(beam)}\")\n",
    "            print(f\"Best complexity so far: {best_state.complexity:.1f}\")\n",
    "        \n",
    "        # Get all possible next states from the current beam\n",
    "        next_beam = []\n",
    "        operations_tried = 0\n",
    "        operations_succeeded = 0\n",
    "        \n",
    "        for current_state in beam:\n",
    "            # Try each possible operation\n",
    "            for op_type, new_text, op_details in get_possible_operations(current_state):\n",
    "                operations_tried += 1\n",
    "                \n",
    "                # Calculate complexity and meaning preservation\n",
    "                new_complexity = sentence_complexity_score(new_text)\n",
    "                new_meaning = calculate_semantic_similarity(text, new_text)\n",
    "                \n",
    "                # Skip if meaning preservation is below threshold\n",
    "                if new_meaning < meaning_threshold:\n",
    "                    if verbose:\n",
    "                        print(f\"Skipping {op_type} operation: meaning preservation too low ({new_meaning:.2f} < {meaning_threshold})\")\n",
    "                    continue\n",
    "                \n",
    "                # Format operation details\n",
    "                detail_str = f\"{op_type}\"\n",
    "                if op_type == \"substitution\":\n",
    "                    detail_str = f\"substituted {len(op_details)} words\"\n",
    "                elif op_type == \"definition\":\n",
    "                    detail_str = f\"added {len(op_details)} definitions\"\n",
    "                elif op_type == \"deletion\":\n",
    "                    detail_str = f\"deleted {len(op_details)} phrases\"\n",
    "                elif op_type == \"splitting\":\n",
    "                    detail_str = f\"split sentences\"\n",
    "                \n",
    "                # Create the new state\n",
    "                new_state = SimplificationState(\n",
    "                    text=new_text,\n",
    "                    complexity=new_complexity,\n",
    "                    meaning_preservation=new_meaning,\n",
    "                    operations_applied=current_state.operations_applied + [(op_type, detail_str)],\n",
    "                    parent=current_state,\n",
    "                    operation_details=op_details\n",
    "                )\n",
    "                \n",
    "                operations_succeeded += 1\n",
    "                \n",
    "                # Add to next beam\n",
    "                next_beam.append(new_state)\n",
    "                \n",
    "                if verbose:\n",
    "                    complexity_change = current_state.complexity - new_complexity\n",
    "                    print(f\"Applied {op_type}: complexity {current_state.complexity:.1f} → {new_complexity:.1f} \" +\n",
    "                          f\"(change: {complexity_change:.1f}), meaning: {new_meaning:.2f}\")\n",
    "                \n",
    "                # Check if this is the best state so far\n",
    "                if target_complexity is not None:\n",
    "                    # If we have a target complexity, prioritize states that reach it\n",
    "                    if new_complexity <= target_complexity:\n",
    "                        if not target_reached or new_meaning > best_state.meaning_preservation:\n",
    "                            best_state = new_state\n",
    "                            target_reached = True\n",
    "                            if verbose:\n",
    "                                print(f\"Target complexity reached: {new_complexity:.1f} ≤ {target_complexity}\")\n",
    "                    # If we haven't reached target yet, prefer states with lower complexity\n",
    "                    elif not target_reached and new_complexity < best_state.complexity:\n",
    "                        best_state = new_state\n",
    "                        if verbose:\n",
    "                            print(f\"New best state: complexity {new_complexity:.1f}\")\n",
    "                else:\n",
    "                    # If no target, prefer states with lower complexity as long as\n",
    "                    # meaning preservation is reasonable\n",
    "                    if new_complexity < best_state.complexity and new_meaning >= meaning_threshold:\n",
    "                        best_state = new_state\n",
    "                        if verbose:\n",
    "                            print(f\"New best state: complexity {new_complexity:.1f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Operations tried: {operations_tried}, succeeded: {operations_succeeded}\")\n",
    "        \n",
    "        # If no operations succeeded, break out of the loop\n",
    "        if operations_succeeded == 0:\n",
    "            if verbose:\n",
    "                print(\"No successful operations found, stopping search\")\n",
    "            break\n",
    "            \n",
    "        # Keep only the top beam_width states for the next iteration\n",
    "        next_beam.sort()  # Sort by priority (__lt__ method)\n",
    "        beam = next_beam[:beam_width]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"New beam size: {len(beam)}\")\n",
    "            print(f\"Best complexity in beam: {min(state.complexity for state in beam):.1f}\")\n",
    "    \n",
    "    # Record total search time\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    # Reconstruct the simplification path\n",
    "    simplification_path = []\n",
    "    state = best_state\n",
    "    while state is not None:\n",
    "        # For the initial state\n",
    "        if not state.operations_applied:\n",
    "            simplification_path.append({\n",
    "                'operation': 'original',\n",
    "                'details': 'original text',\n",
    "                'text': state.text,\n",
    "                'complexity': state.complexity,\n",
    "                'meaning_preservation': state.meaning_preservation\n",
    "            })\n",
    "        else:\n",
    "            # For states with operations\n",
    "            last_op = state.operations_applied[-1]\n",
    "            simplification_path.append({\n",
    "                'operation': last_op[0],\n",
    "                'details': last_op[1],\n",
    "                'text': state.text,\n",
    "                'complexity': state.complexity,\n",
    "                'meaning_preservation': state.meaning_preservation,\n",
    "                'operation_details': state.operation_details\n",
    "            })\n",
    "        state = state.parent\n",
    "    \n",
    "    # Reverse to get chronological order\n",
    "    simplification_path.reverse()\n",
    "    \n",
    "    return {\n",
    "        'original_text': text,\n",
    "        'simplified_text': best_state.text,\n",
    "        'original_complexity': original_complexity,\n",
    "        'simplified_complexity': best_state.complexity,\n",
    "        'complexity_reduction': original_complexity - best_state.complexity,\n",
    "        'meaning_preservation': best_state.meaning_preservation,\n",
    "        'simplification_path': simplification_path,\n",
    "        'operations_count': len(best_state.operations_applied),\n",
    "        'target_complexity_reached': target_reached,\n",
    "        'search_time': search_time\n",
    "    }\n",
    "\n",
    "# Test the improved tree search simplification\n",
    "print(\"Testing improved tree search simplification:\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"The ubiquitous nature of smartphones has revolutionized modern communication.\",\n",
    "    \"The protagonist's epiphany led to a profound transformation in his worldview.\",\n",
    "    \"The government implemented austerity measures to address the fiscal deficit.\",\n",
    "    \"This is the quintessential understanding for text simplification\",\n",
    "    \"This was an absolutely smashing success and I wish for nothing but the best for you two\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {sentence}\")\n",
    "    \n",
    "    # First print the word-by-word complexity\n",
    "    complex_words = identify_complex_words(sentence, threshold=20)  # Using high complexity threshold\n",
    "    print(\"Word complexities:\")\n",
    "    for word, score in sorted(complex_words, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - '{word}': {score:.1f}\")\n",
    "    \n",
    "    # Try with different complexity targets\n",
    "    for config in [\n",
    "        {\"name\": \"Low complexity target\", \"target_complexity\": 15, \"meaning_threshold\": 0.5, \"verbose\": False},\n",
    "        {\"name\": \"Medium complexity target\", \"target_complexity\": 20, \"meaning_threshold\": 0.6, \"verbose\": False},\n",
    "        {\"name\": \"High complexity target\", \"target_complexity\": 40, \"meaning_threshold\": 0.7, \"verbose\": False}\n",
    "    ]:\n",
    "        print(f\"\\n  Config: {config['name']}\")\n",
    "        \n",
    "        result = improved_tree_search_simplification(\n",
    "            sentence,\n",
    "            target_complexity=config[\"target_complexity\"],\n",
    "            meaning_threshold=config[\"meaning_threshold\"],\n",
    "            verbose=config[\"verbose\"],\n",
    "            word_complexity_threshold=20  # Using high complexity threshold\n",
    "        )\n",
    "        \n",
    "        print(f\"  Simplified: {result['simplified_text']}\")\n",
    "        print(f\"  Complexity: {result['original_complexity']:.1f} → {result['simplified_complexity']:.1f}\")\n",
    "        print(f\"  Meaning preservation: {result['meaning_preservation']:.2f}\")\n",
    "        print(f\"  Operations: {result['operations_count']}\")\n",
    "        \n",
    "        # Print the simplification path\n",
    "        if result['operations_count'] > 0:\n",
    "            print(\"\\n  Simplification path:\")\n",
    "            for step_idx, step in enumerate(result['simplification_path']):\n",
    "                if step_idx == 0:\n",
    "                    print(f\"    Step {step_idx}: Original\")\n",
    "                else:\n",
    "                    print(f\"    Step {step_idx}: Applied {step['operation']} - {step['details']}\")\n",
    "                    print(f\"      New complexity: {step['complexity']:.1f}\")\n",
    "                    print(f\"      Meaning preservation: {step['meaning_preservation']:.2f}\")\n",
    "                    \n",
    "                    # Display operation details for better understanding\n",
    "                    if step['operation'] == 'substitution' and step['operation_details']:\n",
    "                        print(\"      Word substitutions:\")\n",
    "                        for orig, subst, orig_score, subst_score in step['operation_details']:\n",
    "                            print(f\"        '{orig}' → '{subst}' ({orig_score:.1f} → {subst_score:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "ADVANCED TEXT SIMPLIFIER WITH DIAGNOSTICS\n",
      "================================================================================\n",
      "Enter a text to simplify, or 'q' to quit.\n",
      "\n",
      "Your text: The government implemented austerity measures to address the fiscal deficit.\n",
      "\n",
      "ANALYZING TEXT COMPLEXITY:\n",
      "Overall complexity: 59.6\n",
      "\n",
      "Word-by-word complexity (most complex first):\n",
      "\n",
      "Target complexity (0-100, or press enter for maximum simplification): 30\n",
      "Preserve meaning? (y/n, default: y): n\n",
      "Word complexity threshold (default: 50): 20\n",
      "Show detailed diagnostics? (y/n, default: n): y\n",
      "\n",
      "Processing...\n",
      "Original text complexity: 59.6\n",
      "Found 5 complex words above threshold 20.0\n",
      "  - 'implemented': 34.0\n",
      "  - 'austerity': 30.0\n",
      "  - 'government': 29.0\n",
      "  - 'deficit': 23.0\n",
      "  - 'measures': 22.0\n",
      "\n",
      "Iteration 1/5\n",
      "Current beam size: 1\n",
      "Best complexity so far: 59.6\n",
      "Found word substitution operation with 4 substitutions\n",
      "Applied substitution: complexity 59.6 → 47.8 (change: 11.8), meaning: 0.58\n",
      "New best state: complexity 47.8\n",
      "Operations tried: 1, succeeded: 1\n",
      "New beam size: 1\n",
      "Best complexity in beam: 47.8\n",
      "\n",
      "Iteration 2/5\n",
      "Current beam size: 1\n",
      "Best complexity so far: 47.8\n",
      "Found word substitution operation with 1 substitutions\n",
      "Applied substitution: complexity 47.8 → 46.0 (change: 1.8), meaning: 0.58\n",
      "New best state: complexity 46.0\n",
      "Operations tried: 1, succeeded: 1\n",
      "New beam size: 1\n",
      "Best complexity in beam: 46.0\n",
      "\n",
      "Iteration 3/5\n",
      "Current beam size: 1\n",
      "Best complexity so far: 46.0\n",
      "Operations tried: 0, succeeded: 0\n",
      "No successful operations found, stopping search\n",
      "\n",
      "RESULTS:\n",
      "Original: The government implemented austerity measures to address the fiscal deficit.\n",
      "Simplified: The regime apply austerity bar to address the fiscal dearth.\n",
      "Complexity: 59.6 → 46.0\n",
      "Complexity reduction: 13.6 points (22.8%)\n",
      "Meaning preservation: 0.58\n",
      "Search time: 0.01 seconds\n",
      "\n",
      "Simplification steps:\n",
      "  Step 0: Original text (complexity: 59.6)\n",
      "  Step 1: Applied substitution - substituted 4 words\n",
      "    New complexity: 47.8 (reduction: 11.8)\n",
      "    Meaning preservation: 0.58\n",
      "    Word substitutions:\n",
      "      'implemented' → 'apply' (34.0 → 16.0)\n",
      "      'government' → 'regime' (29.0 → 18.0)\n",
      "      'deficit' → 'shortage' (23.0 → 22.0)\n",
      "      'measures' → 'bar' (22.0 → 9.0)\n",
      "  Step 2: Applied substitution - substituted 1 words\n",
      "    New complexity: 46.0 (reduction: 1.8)\n",
      "    Meaning preservation: 0.58\n",
      "    Word substitutions:\n",
      "      'shortage' → 'dearth' (22.0 → 15.0)\n",
      "\n",
      "Complex words remaining in simplified text:\n",
      "\n",
      "Would you like to try another example? (Press enter to continue or 'q' to quit)\n",
      "q\n",
      "Thank you for using the Advanced Text Simplifier!\n"
     ]
    }
   ],
   "source": [
    "# Improved Interactive Text Simplifier\n",
    "def improved_simplify_user_text():\n",
    "    \"\"\"Enhanced interactive function with detailed diagnostics\"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED TEXT SIMPLIFIER WITH DIAGNOSTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Enter a text to simplify, or 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        text = input(\"\\nYour text: \")\n",
    "        if text.lower() == 'q':\n",
    "            break\n",
    "            \n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        # First show detailed complexity analysis\n",
    "        print(\"\\nANALYZING TEXT COMPLEXITY:\")\n",
    "        complex_words = identify_complex_words(text, threshold=50)  # Lower threshold for analysis\n",
    "        overall_complexity = sentence_complexity_score(text)\n",
    "        print(f\"Overall complexity: {overall_complexity:.1f}\")\n",
    "        \n",
    "        # Print word-by-word complexity for clear diagnostics\n",
    "        print(\"\\nWord-by-word complexity (most complex first):\")\n",
    "        for word, score in sorted(complex_words, key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  - '{word}': {score:.1f}\" + (\" [COMPLEX]\" if score > 55 else \"\"))\n",
    "        \n",
    "        # Get simplification parameters\n",
    "        target = input(\"\\nTarget complexity (0-100, or press enter for maximum simplification): \")\n",
    "        target_complexity = float(target) if target else None\n",
    "        \n",
    "        preserve = input(\"Preserve meaning? (y/n, default: y): \").lower()\n",
    "        preserve_meaning = preserve != 'n'\n",
    "        \n",
    "        # Set meaning threshold based on preserve_meaning choice\n",
    "        meaning_threshold = 0.6 if preserve_meaning else 0.4\n",
    "        \n",
    "        # Get word complexity threshold\n",
    "        word_thresh = input(\"Word complexity threshold (default: 50): \")\n",
    "        word_complexity_threshold = float(word_thresh) if word_thresh else 50\n",
    "        \n",
    "        verbose = input(\"Show detailed diagnostics? (y/n, default: n): \").lower()\n",
    "        verbose_mode = verbose == 'y'\n",
    "        \n",
    "        print(\"\\nProcessing...\")\n",
    "        \n",
    "        # Run improved tree search with corrected parameters\n",
    "        result = improved_tree_search_simplification(\n",
    "            text,\n",
    "            target_complexity=target_complexity,\n",
    "            verbose=verbose_mode,\n",
    "            meaning_threshold=meaning_threshold,  # Use the threshold we determined\n",
    "            beam_width=5,  # Use wider beam\n",
    "            word_complexity_threshold=word_complexity_threshold  # Use user's threshold\n",
    "        )\n",
    "        \n",
    "        # Show detailed results\n",
    "        print(\"\\nRESULTS:\")\n",
    "        print(f\"Original: {result['original_text']}\")\n",
    "        print(f\"Simplified: {result['simplified_text']}\")\n",
    "        print(f\"Complexity: {result['original_complexity']:.1f} → {result['simplified_complexity']:.1f}\")\n",
    "        print(f\"Complexity reduction: {result['complexity_reduction']:.1f} points \" +\n",
    "              f\"({result['complexity_reduction']/max(0.1, result['original_complexity'])*100:.1f}%)\")\n",
    "        print(f\"Meaning preservation: {result['meaning_preservation']:.2f}\")\n",
    "        print(f\"Search time: {result['search_time']:.2f} seconds\")\n",
    "        \n",
    "        # Show each step and its impact\n",
    "        if len(result['simplification_path']) > 1:  # If we made any changes\n",
    "            print(\"\\nSimplification steps:\")\n",
    "            for i, step in enumerate(result['simplification_path']):\n",
    "                if i == 0:\n",
    "                    print(f\"  Step {i}: Original text (complexity: {step['complexity']:.1f})\")\n",
    "                else:\n",
    "                    print(f\"  Step {i}: Applied {step['operation']} - {step['details']}\")\n",
    "                    print(f\"    New complexity: {step['complexity']:.1f} \" + \n",
    "                          f\"(reduction: {result['simplification_path'][i-1]['complexity'] - step['complexity']:.1f})\")\n",
    "                    print(f\"    Meaning preservation: {step['meaning_preservation']:.2f}\")\n",
    "                    \n",
    "                    # Show specific details for different operation types\n",
    "                    if step['operation'] == 'substitution' and step['operation_details']:\n",
    "                        print(\"    Word substitutions:\")\n",
    "                        for orig, subst, orig_score, subst_score in step['operation_details']:\n",
    "                            print(f\"      '{orig}' → '{subst}' ({orig_score:.1f} → {subst_score:.1f})\")\n",
    "                    \n",
    "                    elif step['operation'] == 'deletion' and step['operation_details']:\n",
    "                        print(\"    Content deleted:\")\n",
    "                        for content, type_info in step['operation_details'][:3]:  # Show first 3 deletions\n",
    "                            print(f\"      '{content}' ({type_info})\")\n",
    "        else:\n",
    "            print(\"\\nNo simplification operations were applied.\")\n",
    "        \n",
    "        # Analyze the simplified text words\n",
    "        if result['simplified_text'] != result['original_text']:\n",
    "            print(\"\\nComplex words remaining in simplified text:\")\n",
    "            remaining_complex = identify_complex_words(result['simplified_text'], threshold=50)\n",
    "            for word, score in sorted(remaining_complex, key=lambda x: x[1], reverse=True):\n",
    "                if score > 55:  # Only show words still above threshold\n",
    "                    print(f\"  - '{word}': {score:.1f}\")\n",
    "        \n",
    "        print(\"\\nWould you like to try another example? (Press enter to continue or 'q' to quit)\")\n",
    "        if input().lower() == 'q':\n",
    "            break\n",
    "    \n",
    "    print(\"Thank you for using the Advanced Text Simplifier!\")\n",
    "\n",
    "# Run the improved interactive simplifier\n",
    "improved_simplify_user_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
